{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2b792a-2131-4bcd-bc78-7d85eb745a00",
   "metadata": {},
   "source": [
    "# Filter Methods In Feature Selection\n",
    "\n",
    "## Mutual Information\n",
    "\n",
    "- Mutual information is a filter method\n",
    "- It selected features only on their relationship with the target, not on any trained model.\n",
    "- A feature is good if it tells you a lot about the target.\n",
    "- How much knowing X reduces uncertainty about Y.\n",
    "- Feature and target are independent → MI=0\n",
    "- Strong dependence → MI is large.\n",
    "\n",
    "$$\n",
    "I(X,Y) = \\sum_{x \\in X}^{} \\sum_{y \\in Y}^{} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}\n",
    "$$\n",
    "\n",
    "- if features are independent then MI will be 0.\n",
    "\n",
    "$$\n",
    "H(Y) = - \\sum_{y}^{} p(y) \\log p(y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(Y|X) = - \\sum_{x,y}^{} p(x,y) \\log p(y|x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "I(X;Y) = H(Y) - H(Y|X) \n",
    "$$\n",
    "\n",
    "So, MI = Reduction in uncertainty about Y when X is known. \n",
    "\n",
    "## ANOVA For Feature Selection\n",
    "\n",
    "- Identify features that differ significantly across target classes.\n",
    "- Used mainly for classification tasks with a categorical target.\n",
    "- Does the mean of the feature vary between classes.\n",
    "- Anova computes the ratio of variance between classes to variance within classes.\n",
    "\n",
    "$$\n",
    "F = \\frac{\\text{Variance between classes}}{\\text{Variance within classes}}\n",
    "$$\n",
    "\n",
    "- Algorithm\n",
    "    - We have a feature $X_j$\n",
    "    - Classes $C_1, C_2, …, C_k$\n",
    "    - Number of samples n_i in class i\n",
    "    - $\\bar{x}_i$ be the mean of feature in class i\n",
    "    - $\\bar{x}$ be the overall mean of the feature\n",
    "    - Compute the between and within class variance and then the f statistic using the following equations.\n",
    "\n",
    "$$\n",
    "SSB_j = \\sum_{i=1}^{k} n_i (\\bar{x}_i - \\bar{x})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "SSW_j = \\sum_{i=}^{k} \\sum_{x \\in C_i}^{} (x-\\bar{x}_i)^2\n",
    "$$\n",
    "\n",
    "Between class sum of squares measures how much the class means deviate from overall mean. \n",
    "\n",
    "Within class sum of squares measure the variability inside each class. \n",
    "\n",
    "$$\n",
    "F_j = \\frac{SSB_j/(k-1)}{SSW_j/(N-k)}\n",
    "$$\n",
    "\n",
    "- High F_j → Feature strongly separates classes\n",
    "- Low F_j → Feature weakly separates classes\n",
    "- This method works well when target is categorical.\n",
    "- It only captures differences in mean ( ignores variance structure, interactions)\n",
    "\n",
    "## Variation Inflation Factor - VIF\n",
    "\n",
    "- High VIF → Feature is highly correlated with others → May consider removing it.\n",
    "\n",
    "$$\n",
    "VIF_j = \\frac{1}{1-R_j^2}\n",
    "$$\n",
    "\n",
    "- If X_j feature is highly predictable from other features → High VIF.\n",
    "- 1<VIF<5 → Moderate correlation\n",
    "- VIF>5 → Higher correlation, consider removing\n",
    "\n",
    "## Weight of Evidence\n",
    "\n",
    "How strongly a feature value ( or bin) separates good vs bad customers.\n",
    "\n",
    "$$\n",
    "\\text{Weight of Evidence} = \\log \\left(\\frac{\\text{\\% of Good in the bin}} {\\text{\\% of Bad in the bin}} \\right)\n",
    "$$\n",
    "\n",
    "- Is this bin more associated with good or bad outcome?\n",
    "- For continuous features we can use equal width bins or optimal binning.\n",
    "- We count the % of Good and % Bad .\n",
    "- For each bin we get the percentage.\n",
    "- Final computation.\n",
    "- If WoE >0 → More good than bad\n",
    "- After this, we can compute the Information Value ;\n",
    "\n",
    "$$\n",
    "IV = \\sum_{i} (\\text{\\% of Good} - \\text{\\% of Bad}) * WoE\n",
    "$$\n",
    "\n",
    "- if IV>0.3 then the predictor is strong\n",
    "- if IV <0.1 its a weak predictor\n",
    "\n",
    "## Chi-Square Test of Independence\n",
    "\n",
    "- A feature is useful , when its distribution is different across classes.\n",
    "- A feature is informative when it is dependent of target.\n",
    "- Null Hypothesis : X & Y are independent\n",
    "- We create the contingency table.\n",
    "- We compute the expected counts\n",
    "- Compute the chi-square statistic\n",
    "\n",
    "$$\n",
    "E_{ij} = \\frac{(\\text{Row i Total}) * (\\text{Column j Total})}{N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Chi^2 = \\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n",
    "$$\n",
    "\n",
    "If we have r categories in the feature and c classes in the target column, then the degrees of freedom will be $(r-1) (c-1)$ \n",
    "\n",
    "# Wrapper Methods in Feature Selection\n",
    "\n",
    "- Iterative procedures\n",
    "- One independent variable at a time is added or deleted based on the p-value.\n",
    "\n",
    "## Forward Selection\n",
    "\n",
    "Pick the feature that improves performance the most\n",
    "\n",
    "- For each candidate feature:\n",
    "    - Add it to current feature set\n",
    "    - Train model\n",
    "    - Evaluate with validation/cv\n",
    "- Compare scores\n",
    "- Choose the feature giving highest score\n",
    "- Fix it and move to next round\n",
    "\n",
    "## Backward Selection\n",
    "\n",
    "Start big, remove one at a time, and keep the removal that hurts performance the least.\n",
    "\n",
    "## Stepwise Selection\n",
    "\n",
    "Hybrid of forward selection and backward elimination.\n",
    "\n",
    "Forward selection problem:\n",
    "\n",
    "- Once you add a feature , it never gets removed\n",
    "- But later, it might become redundant when new features are added.\n",
    "\n",
    "Stepwise fixes this by adding good features but also kick out bad ones when they stop helping. \n",
    "\n",
    "We add the features and in each step we do a backward elimination check. \n",
    "\n",
    "## Recursive Feature Elimination\n",
    "\n",
    "- In backward elimination the removal is based on; Which feature can I remove with the least drop in model performance?\n",
    "- In RFE the removal is based on which feature is least important according to the model itself ?\n",
    "- so we train model once, get the feature importance scores, coefficients , then remove the weakest, retrain and repeat.\n",
    "- Backward Elimination: If I remove you, what happens to performance?\n",
    "- RFE : How important does the model think you are?\n",
    "\n",
    "# Embedded Methods in Feature Selection\n",
    "\n",
    "## Lasso - L1 Regularisation\n",
    "\n",
    "- Feature selection happens automatically during training.\n",
    "- Add a penalty that pushes coefficients to exactly zero.\n",
    "- L1 penalty has a diamond shaped constraint.\n",
    "- Optimal solution often hits the corners.\n",
    "\n",
    "$$\n",
    "\\lambda \\sum_j |\\beta_j|\n",
    "$$\n",
    "\n",
    "- As lambda increases small coefficients shrink to 0 first.\n",
    "- Non-zero coefficients = selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cccdee-69fc-4c87-a7ef-b929150d2698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

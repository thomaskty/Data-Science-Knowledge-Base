{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3711d4-ae1b-4cc3-aec6-d32f49890697",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4234dc-1e98-48f3-ad9c-aa35567f1dea",
   "metadata": {},
   "source": [
    "## Foundation \n",
    " - Information retrieval - fetches relevant context\n",
    " - Generative models - produce grounded responses.\n",
    " - Reduces hallucinations\n",
    " - Allow LLMs to access private or up-to-date knowledge.\n",
    " - **Indexing Phase** :\n",
    "    - **Data Ingestion -> Chunking -> Embedding -> Vector Storage**\n",
    "    - Chunking Methods\n",
    "    - Vector Storage Options : Pinecone/Weaviate/FAISS\n",
    " - **Retrieval Phase** :\n",
    "    - **Query -> Embedding -> Vector search ->Similarity -> Topk results**\n",
    "    - Retrieval Types\n",
    "    - Beam Search\n",
    " - **Generation Phase** :\n",
    "    - Retrieved context is injected into a structured prompt.\n",
    "    - Generating the final answer.\n",
    " - Embedding Models\n",
    "    - OpenAI text-embedding\n",
    "    - BERT,SBERT\n",
    "    - Cohere Embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac244f21-6375-4e89-8d2f-8bdab592aa6e",
   "metadata": {},
   "source": [
    "## Rag Evaluation\n",
    " - RAG must be evaluated in three separate stages: 1. retrieval quality, generation quality, end-to-end task performance; \n",
    " - Libraries : RAGAS,TrueLens, Promptfoo\n",
    "\n",
    "**Retrieval evaluation measures** whether the system fetched the right chunks before generation.\n",
    "$$\n",
    "\\text{Recall@k} = \\frac{\\text{\\# of relevant docs retrieved in topk}}{\\text{Total \\# relevant docs}}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Precision@k} = \\frac{\\text{\\# of relevant docs retrieved in topk}}{\\text{Total \\# retrieved docs,k}}\n",
    "$$\n",
    "\n",
    "**Generation Evaluation Metrics**: This evaluates the quality of the models' final answer after combining retrieved context.\n",
    " - Faithfulness : Is the answer hallucination free ? \n",
    " - Groundedness : Is the answer fully supported by retrieved context? \n",
    " - Answer Relevance : Check whether the generated answer addresses the user's query.\n",
    " - Correctness : Is the answer factually right ? Does it match a known correct answer?\n",
    " - Toxicity: Checking the policy violations, PII leakage, Offensive language.\n",
    "\n",
    "**Product Experience Metrics**:\n",
    " - Task Success Rate : Does the RAG system solve the user's problem?\n",
    " - User Satisfaction :\n",
    " - Latency : Time taken for the lookup, first token generation.\n",
    " - Cost Per Query : Embedding cost + Storage Cost + Generation Cost.\n",
    "\n",
    "**Evaluation Methods**\n",
    "- Offline Evaluation(Development): using ground truth.\n",
    "- Online Evaluation (After deployment) : Tracking/Feedback/A-B Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee40386-7424-4726-ad53-809333d982f9",
   "metadata": {},
   "source": [
    "# Prompting Engineering\n",
    "\n",
    "## Types of Prompting \n",
    "* **Zero Shot Prompting**\n",
    "    - Giving the model a task without any example.\n",
    "    - relies on the model's pretrained knowledge to perform a task without examples.\n",
    "    - Example : 'Summarize this paragraph'\n",
    "* **One-Shot Prompting**\n",
    "    - You give one example to show the model the pattern.\n",
    "* **Few Shot Prompting**\n",
    "    - Giving multiple examples to teach the model the structure or style.\n",
    "* **Chain-of-Thought Prompting**\n",
    "    - Asking the model to think step by step\n",
    "    - Improves logical reasoning and complex problem-solving\n",
    "* **Self-Consistency Prompting**\n",
    "    - Getting multiple chain-of-thought answer and picking the most common one.\n",
    "    - Improves reasoning realiability\n",
    "* **Role Prompting**\n",
    "    - Assigning a role to the model so it behaves like a specific expert.\n",
    "* **Instruction Prompting**\n",
    "    - Clear,direct instructions telling the model exactly what to do\n",
    "* **ReAct Prompting - Reason + Act**\n",
    "    - Model alternates between reasoning steps and taking actions.\n",
    "    - used in agents\n",
    "* **Retrieval-Augmented Prompting**\n",
    "    - You pass external documents or context into the prompt ( used in rag)\n",
    " \n",
    "## Structure of a good Prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbcd073-a230-49fa-afcc-7ec4ea5b2683",
   "metadata": {},
   "source": [
    "* Clear → No ambiguity\n",
    "* Specific → Defines scope\n",
    "* Context-rich → Gives necessary background\n",
    "* Instructional → Contains a clear verb (Explain, Generate, Evaluate, Translate…)\n",
    "* Constrained → Boundaries, rules, or format\n",
    "* Modular → Easy to reuse or extend\n",
    "* Deterministic → Minimizes randomness\n",
    "* Evaluatable → Output format that can be checked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c4d25-3dfa-48ef-be83-bf9156a70f0a",
   "metadata": {},
   "source": [
    "#  Retrieval Pipeline\n",
    "* **Step 1 : Query Processing**\n",
    "* **Step 2 : Query Embedding** - use an embedding model \n",
    "* **Step 3 : Document Chunking** \n",
    "    * **Chunking strategies**\n",
    "        - Fixed size chunks (e.g., 512 tokens)\n",
    "        - Sentence-based\n",
    "        - Paragraph-based\n",
    "        - Overlapping chunks\n",
    "        - Semantic chunking (LLM-chunking)\n",
    "* **Step 4 : Store the Document Embedding in a vector database**\n",
    "   - Store chunk text,metadata,embedding vector\n",
    "   - Multi-Vector-Embedding : ColBERT\n",
    "     - Multi-vector search stores multiple vectors for a single chunk\n",
    "* **Step 5 : Retrieval**\n",
    "   - Dense Retriever Stage : Use similarity search to retrieve topk chunks\n",
    "   - Hybrid Reriever: Vector search sometimes misses exact keywords,BM25 fails at semantic meaning.\n",
    "      - so keyword search + vector search\n",
    "      - Merge the results : Reciprocal Rank Fusion / Weighted Score Fusion\n",
    "* **Step6 : Reranking**\n",
    "  - Ranking the retrieved chunks again using a re-ranker.\n",
    "  - A cross encoder is a bert-style transformer takes the query and candidate chunk, outputs a relevance score. This is much more accurate than vector similarity.\n",
    "* **Metadata-Filtering**\n",
    "  - restricting retrieval using structured fields like document type, version,date, producid,..\n",
    "  - It ensures that retrieval considers relevant documents.\n",
    "  - We can apply filters before retrieval to narrow the search space or after retrieval to clean up results.\n",
    "* **Context Assembly**\n",
    "  - Merge relevant chunks\n",
    "  - Remove duplicates\n",
    "* **LLM Prompt Construction**\n",
    "  - System Prompt\n",
    "  - Instructions\n",
    "  - Frame the Context\n",
    "  - User Query\n",
    "* **Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c07038-2927-4e44-99e4-50440b2901cd",
   "metadata": {},
   "source": [
    "# Fine Tuning Topics\n",
    "- **Full Fine Tuning**\n",
    "  - Training the entire model on new domain specific data.\n",
    "  - Takes a pre-trained model\n",
    "  - Continuous training\n",
    "  - Updates all the weights\n",
    "  - requires substantial computational resources\n",
    "  - when maximum performance on a specific task is required.\n",
    "  - specialized models for critical apps\n",
    "    \n",
    "- **Parameter Efficient Fine-Tuning (PEFT)**: Fine tune only a small subset of model parameters, while keeping the majority frozen. \n",
    "   - LoRa : Low Rank Adaptation:\n",
    "     - Adds small adaptors to model layers\n",
    "     - only trains these low-rank matrices\n",
    "     - can be merged back into base model after training. \n",
    "   - QLoRA : Quantized Lora\n",
    "     - Adds quantization to lora\n",
    "     - uses 4 bit precision for base model + lora adapters\n",
    "     - extreamly memory efficient - can run on consumer gpu.\n",
    "\n",
    "- **Fine-Tuning when**\n",
    "  - specific style/tone adaptation.\n",
    "  - specialized reasoning patterns - code generation llms, proofs,...\n",
    "  - base model lacks domain knowledge and thats not easily retrievable.\n",
    "  - latency is critical ( rag has additional retrieval step)\n",
    "\n",
    "- **Rag when**:\n",
    "  - when we want to avoid hallucination\n",
    "  - combining information from multiple sources.\n",
    "  - Data privacy is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c914da53-24fc-470c-87d0-dd7a5298d00e",
   "metadata": {},
   "source": [
    "# LLM - Inference Parameters\n",
    "\n",
    "* temperature : controls the randomness in token selection : lower temperature gives more deterministic\n",
    "* top p :\n",
    "* top K :\n",
    "* max tokens :\n",
    "* frequency penalty:\n",
    "* seed\n",
    "* beam search\n",
    "* minimum new tokens:\n",
    "\n",
    "| Parameter                | Controls              | Typical Range | Best For                   |\n",
    "| ------------------------ | --------------------- | ------------- | -------------------------- |\n",
    "| **Temperature**          | Creativity            | 0–1.2         | Tone/style + randomness    |\n",
    "| **Top-p**                | Probability mass      | 0.8–1.0       | Balanced sampling          |\n",
    "| **Top-k**                | # of candidate tokens | 20–200        | Precise randomness control |\n",
    "| **Max tokens**           | Output length         | app-dependent | Cost & length control      |\n",
    "| **Repetition penalty**   | Avoid loops           | 1.05–1.3      | Avoiding repetition        |\n",
    "| **Frequency/P. penalty** | Token frequency       | 0–1.5         | Variety in reply           |\n",
    "| **Stop sequences**       | Termination           | n/a           | Structured output          |\n",
    "| **Logit bias**           | Token forcing         | n/a           | JSON, safety, formatting   |\n",
    "| **Seed**                 | Determinism           | int           | Reproducible outputs       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d89425-ed4d-438c-ada8-7ce4b94af9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projects \n",
    "# summarization of model development document, and corresponding rag\n",
    "# model from sivakumar : ask for model id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9cbce-4be8-46e0-9b63-ea4282e0140e",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a7999-590b-4644-8b91-4ff6e7b6ff8f",
   "metadata": {},
   "source": [
    "An LLM-powered entity that can plan, reason, take actions using tools, and interact with an environment to complete tasks autonomously.\n",
    "\n",
    "## Abilities\n",
    "* plan -> break task into steps\n",
    "* use tools -> apis,databases,code executors\n",
    "* observe results -> feedback loop\n",
    "* adjust behavior -> reflection\n",
    "* communicate with other agent -> collaboration\n",
    "\n",
    "## Types\n",
    "* reflex agents : simple , rule based , if x happens do y. This is used in monitoring and alerts.\n",
    "* stateful agents: maintain memory, conversation, previous step, long term user preference.\n",
    "\n",
    "## Agentic Workflows\n",
    "* Agents chained together with dependencies.\n",
    "* Research Agent -> Analysis agent -> Writing agent -> QA Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e66667a-fcdf-43f2-b722-ca18e0861653",
   "metadata": {},
   "source": [
    "## Creating single agents\n",
    "\n",
    "1. A Role : Persona: It defines the expertise. \n",
    "2. Abilities/Tools : toolset , the agent has access to.\n",
    "3. Reasoning Pattern : CoT, ReAct, Plan -> Execute, Reflect -> Improve.\n",
    "4. Control Loop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23329a-46d6-4014-9e61-fe177d8dd92e",
   "metadata": {},
   "source": [
    "## Multi-Agent Systems:\n",
    "\n",
    "Contains multiple agents collaborating like a team. Each aget will be having the following elements;\n",
    "1. has a specific role\n",
    "2. communicate with others agents\n",
    "3. can critique others' outputs\n",
    "4. can share memory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da2739-228b-4f88-95ec-b030e5447e72",
   "metadata": {},
   "source": [
    "## Key Concepts in Agent Orchestration\n",
    "* Task Decomposition - Breaking the task into smaller subtasks\n",
    "* Tool Routing - Choosing the right tool dynamically\n",
    "* Memory Management - Long term memory, short-term scratchpads,shared \n",
    "* safety + guardrails - prevent loops, bad api calls, dangerous actions\n",
    "* error recovery - failed tool calls, missing info, ambiguous requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f79341-c47f-4a6e-b2aa-f90b4a1baa67",
   "metadata": {},
   "source": [
    "# Token Throughput\n",
    "\n",
    "* Input Throughput : How fast the model can ingest tokens\n",
    "* Output Throughput : How fast the model can generate tokes.\n",
    "* High token throughput means, faster responses.\n",
    "\n",
    "## Factors affecting throughput\n",
    "* Model size\n",
    "* Hardware\n",
    "* Parallelization strategies\n",
    "* Quantizatio level ( 4bit,8bit)\n",
    "* Inference engine\n",
    "* Batch size\n",
    "\n",
    "\n",
    "Throughput is one metric for LLM serving performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff1939-3a43-4d89-a672-9c4a0f33d977",
   "metadata": {},
   "source": [
    "# Latency\n",
    "* Total time from sending a prompt → receiving a response.\n",
    "* This includes:\n",
    "  * Queueing time\n",
    "  * Tokenization\n",
    "  * Model inference time\n",
    "  * Output token streaming\n",
    "  * Tool use delays\n",
    "  * Network overhead\n",
    "* Techniques to reduce latency:\n",
    "  * Model quantization\n",
    "  * smaller models -slms\n",
    "  * distilled models\n",
    "  * optimized inference engines ( TensorRT-LLM, vLLM)\n",
    "  * system design techniques\n",
    "\n",
    "High Throughput + low latency = optimal user experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3031090-1315-47ff-8826-8aa84e3feb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d4f0f-f6b1-4770-aabd-4731b0aa3869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

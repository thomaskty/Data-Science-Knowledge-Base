{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4234dc-1e98-48f3-ad9c-aa35567f1dea",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "## Foundation \n",
    " - Information retrieval - fetches relevant context\n",
    " - Generative models - produce grounded responses.\n",
    " - Reduces hallucinations\n",
    " - Allow LLMs to access private or up-to-date knowledge.\n",
    " - **Indexing Phase** :\n",
    "    - **Data Ingestion -> Chunking -> Embedding -> Vector Storage**\n",
    "    - Chunking Methods\n",
    "    - Vector Storage Options : Pinecone/Weaviate/FAISS\n",
    " - **Retrieval Phase** :\n",
    "    - **Query -> Embedding -> Vector search ->Similarity -> Topk results**\n",
    "    - Retrieval Types\n",
    "    - Beam Search\n",
    " - **Generation Phase** :\n",
    "    - Retrieved context is injected into a structured prompt.\n",
    "    - Generating the final answer.\n",
    " - Embedding Models\n",
    "    - OpenAI text-embedding\n",
    "    - BERT,SBERT\n",
    "    - Cohere Embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac244f21-6375-4e89-8d2f-8bdab592aa6e",
   "metadata": {},
   "source": [
    "## Rag Evaluation\n",
    " - RAG must be evaluated in three separate stages: 1. retrieval quality, generation quality, end-to-end task performance; \n",
    " - Libraries : RAGAS,TrueLens, Promptfoo\n",
    "\n",
    "**Retrieval evaluation measures** whether the system fetched the right chunks before generation.\n",
    "$$\n",
    "\\text{Recall@k} = \\frac{\\text{\\# of relevant docs retrieved in topk}}{\\text{Total \\# relevant docs}}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Precision@k} = \\frac{\\text{\\# of relevant docs retrieved in topk}}{\\text{Total \\# retrieved docs,k}}\n",
    "$$\n",
    "\n",
    "**Generation Evaluation Metrics**: This evaluates the quality of the models' final answer after combining retrieved context.\n",
    " - Faithfulness : Is the answer hallucination free ? \n",
    " - Groundedness : Is the answer fully supported by retrieved context? \n",
    " - Answer Relevance : Check whether the generated answer addresses the user's query.\n",
    " - Correctness : Is the answer factually right ? Does it match a known correct answer?\n",
    " - Toxicity: Checking the policy violations, PII leakage, Offensive language.\n",
    "\n",
    "**Product Experience Metrics**:\n",
    " - Task Success Rate : Does the RAG system solve the user's problem?\n",
    " - User Satisfaction :\n",
    " - Latency : Time taken for the lookup, first token generation.\n",
    " - Cost Per Query : Embedding cost + Storage Cost + Generation Cost.\n",
    "\n",
    "**Evaluation Methods**\n",
    "- Offline Evaluation(Development): using ground truth.\n",
    "- Online Evaluation (After deployment) : Tracking/Feedback/A-B Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee40386-7424-4726-ad53-809333d982f9",
   "metadata": {},
   "source": [
    "# Prompting Engineering\n",
    "\n",
    "## Types of Prompting \n",
    "* **Zero Shot Prompting**\n",
    "    - Giving the model a task without any example.\n",
    "    - relies on the model's pretrained knowledge to perform a task without examples.\n",
    "    - Example : 'Summarize this paragraph'\n",
    "* **One-Shot Prompting**\n",
    "    - You give one example to show the model the pattern.\n",
    "* **Few Shot Prompting**\n",
    "    - Giving multiple examples to teach the model the structure or style.\n",
    "* **Chain-of-Thought Prompting**\n",
    "    - Asking the model to think step by step\n",
    "    - Improves logical reasoning and complex problem-solving\n",
    "* **Self-Consistency Prompting**\n",
    "    - Getting multiple chain-of-thought answer and picking the most common one.\n",
    "    - Improves reasoning realiability\n",
    "* **Role Prompting**\n",
    "    - Assigning a role to the model so it behaves like a specific expert.\n",
    "* **Instruction Prompting**\n",
    "    - Clear,direct instructions telling the model exactly what to do\n",
    "* **ReAct Prompting - Reason + Act**\n",
    "    - Model alternates between reasoning steps and taking actions.\n",
    "    - used in agents\n",
    "* **Retrieval-Augmented Prompting**\n",
    "    - You pass external documents or context into the prompt ( used in rag)\n",
    " \n",
    "## Structure of a good Prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbcd073-a230-49fa-afcc-7ec4ea5b2683",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b92c4d25-3dfa-48ef-be83-bf9156a70f0a",
   "metadata": {},
   "source": [
    "#  Retrieval Pipeline\n",
    "* **Step 1 : Query Processing**\n",
    "* **Step 2 : Query Embedding** - use an embedding model \n",
    "* **Step 3 : Document Chunking** \n",
    "    * **Chunking strategies**\n",
    "        - Fixed size chunks (e.g., 512 tokens)\n",
    "        - Sentence-based\n",
    "        - Paragraph-based\n",
    "        - Overlapping chunks\n",
    "        - Semantic chunking (LLM-chunking)\n",
    "* **Step 4 : Store the Document Embedding in a vector database**\n",
    "   - Store chunk text,metadata,embedding vector\n",
    "   - Multi-Vector-Embedding : ColBERT\n",
    "     - Multi-vector search stores multiple vectors for a single chunk\n",
    "* **Step 5 : Retrieval**\n",
    "   - Dense Retriever Stage : Use similarity search to retrieve topk chunks\n",
    "   - Hybrid Reriever: Vector search sometimes misses exact keywords,BM25 fails at semantic meaning.\n",
    "      - so keyword search + vector search\n",
    "      - Merge the results : Reciprocal Rank Fusion / Weighted Score Fusion\n",
    "* **Step6 : Reranking**\n",
    "  - Ranking the retrieved chunks again using a re-ranker.\n",
    "  - A cross encoder is a bert-style transformer takes the query and candidate chunk, outputs a relevance score. This is much more accurate than vector similarity.\n",
    "* **Metadata-Filtering**\n",
    "  - restricting retrieval using structured fields like document type, version,date, producid,..\n",
    "  - It ensures that retrieval considers relevant documents.\n",
    "  - We can apply filters before retrieval to narrow the search space or after retrieval to clean up results.\n",
    "* **Context Assembly**\n",
    "  - Merge relevant chunks\n",
    "  - Remove duplicates\n",
    "* **LLM Prompt Construction**\n",
    "  - System Prompt\n",
    "  - Instructions\n",
    "  - Frame the Context\n",
    "  - User Query\n",
    "* **Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c07038-2927-4e44-99e4-50440b2901cd",
   "metadata": {},
   "source": [
    "# Fine Tuning Topics\n",
    "- **Full Fine Tuning**\n",
    "  - Training the entire model on new domain specific data.\n",
    "  - Takes a pre-trained model\n",
    "  - Continuous training\n",
    "  - Updates all the weights\n",
    "  - requires substantial computational resources\n",
    "  - when maximum performance on a specific task is required.\n",
    "  - specialized models for critical apps\n",
    "    \n",
    "- **Parameter Efficient Fine-Tuning (PEFT)**: Fine tune only a small subset of model parameters, while keeping the majority frozen. \n",
    "   - LoRa : Low Rank Adaptation:\n",
    "     - Adds small adaptors to model layers\n",
    "     - only trains these low-rank matrices\n",
    "     - can be merged back into base model after training. \n",
    "   - QLoRA : Quantized Lora\n",
    "     - Adds quantization to lora\n",
    "     - uses 4 bit precision for base model + lora adapters\n",
    "     - extreamly memory efficient - can run on consumer gpu.\n",
    "\n",
    "- **Fine-Tuning when**\n",
    "  - specific style/tone adaptation.\n",
    "  - specialized reasoning patterns - code generation llms, proofs,...\n",
    "  - base model lacks domain knowledge and thats not easily retrievable.\n",
    "  - latency is critical ( rag has additional retrieval step)\n",
    "\n",
    "- **Rag when**:\n",
    "  - when we want to avoid hallucination\n",
    "  - combining information from multiple sources.\n",
    "  - Data privacy is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c914da53-24fc-470c-87d0-dd7a5298d00e",
   "metadata": {},
   "source": [
    "# LLM - Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814363e-7cbe-4e4e-91e7-4c1f8737bb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3dc190f-cf73-4c92-b53f-e60629a1924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling\n",
    "# Agents\n",
    "# vLLM\n",
    "# Token throughput\n",
    "# Latency reduction\n",
    "# Model distillation\n",
    "# Model orchestration (vLLM, TensorRT-LLM, Ollama)\n",
    "# slms : how it is useful\n",
    "# mixture of experts\n",
    "# gpt -oss ppt contents\n",
    "# lora/qlora in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d89425-ed4d-438c-ada8-7ce4b94af9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projects \n",
    "# summarization of model development document, and corresponding rag\n",
    "# model from sivakumar : ask for model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967942b8-7111-4254-9563-46b56c3da18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6bafa-acff-405b-ac25-364a989adb37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

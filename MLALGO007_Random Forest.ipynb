{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7d6489-f2a1-48dc-84ce-89534bf7814c",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c66127-19fa-4aeb-ad3d-bca88864683f",
   "metadata": {},
   "source": [
    "Ensemble methods combine multiple models - base learner to produce a single stronger model. \n",
    "\n",
    "- Different models make different mistakes and aggregation smooths them out\n",
    "- Base learners are usually simple ( decision trees, linear models).\n",
    "\n",
    "## Bagging\n",
    "\n",
    "- Bagging focuses on variance reduction\n",
    "- Multiple models are trained independently\n",
    "- Each model sees a different bootstrap sample ( sampling with replacement)\n",
    "- Predictions are combined by averaging or majority vote.\n",
    "- Works best for high variance models ( decision trees)\n",
    "\n",
    "## Boosting\n",
    "\n",
    "- Focuses on bias reduction\n",
    "- Models are trained sequentially\n",
    "- Each new model focuses on previous mistakes\n",
    "- Predictions are combined using weighted sums\n",
    "\n",
    "## Stacking\n",
    "\n",
    "- Combines different types of models\n",
    "- Multiple diverse base models are trained in parallel\n",
    "- Their predictions are used as features for a meta-model\n",
    "- The meta model learns how to weight or combine base predictions.\n",
    "- logistic regression on top of RF + SVM + GBM\n",
    "\n",
    "## Voting\n",
    "\n",
    "Voting combines multiple models using simple aggregation rules\n",
    "\n",
    "- Hard voting : majority class vote\n",
    "- Soft voting : Averaging predicted probabilities\n",
    "- Models are usually diverse\n",
    "\n",
    "---------------\n",
    "- High variance, unstable model → **Bagging / Random Forest**\n",
    "- High bias, underfitting → **Boosting**\n",
    "- Very different model types → **Stacking**\n",
    "- Simple ensemble baseline → **Voting**\n",
    "- Categorical-heavy data → **CatBoost**\n",
    "- Massive dataset, speed matters → **LightGBM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4112c891-3419-4cc9-a8f8-a1d8385959fe",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf2ce5-3e60-4e90-9168-04ffd33e9535",
   "metadata": {},
   "source": [
    "## Bias Variance Trade off\n",
    "\n",
    "Decision trees has low bias and very high variance . Small changes in data → Very different trees. This makes trees unstable and overfitting-prone. How do we keep the low bias of the trees, but reduce variance?\n",
    "\n",
    "**Train many different trees and average them**\n",
    "\n",
    "- outlier detection using isolated forests ( unsupervised method)\n",
    "- Hyper parameters of RandomForest\n",
    "\n",
    "## Randomness in Random Forest\n",
    "\n",
    "- Bootstrap Sampling → Randomly sample with replacement\n",
    "- Feature Subtraction → Column Randomness\n",
    "\n",
    "## Out of Bag Score (OOB Score)\n",
    "\n",
    "- Each tree is trained on a **bootstrap sample** (random sample with replacement) of the data.\n",
    "- About **63%** of the data is used in a tree’s training.\n",
    "- The remaining **~37%** is **not used** for that tree — these are called **Out-of-Bag (OOB)** samples\n",
    "- For each data point, it is predicted by trees that did not see it during training. These predictions are aggregated and compared with the true value → This accuracy is computed and called as OOB Accuracy.\n",
    "\n",
    "## Proximity Matrix\n",
    "\n",
    "Measures how often two data points end up in the same leaf node across all trees in the forest. \n",
    "\n",
    "$$\n",
    "P(i,j) = \\frac{\\text{Number of trees where i \\& j in the same leaf}}{\\text{Total Number of trees}}\n",
    "$$\n",
    "\n",
    "- Gives some kinda similarity measure ( data driven)\n",
    "- If a point has missing values ; then find points that are most similar to it ( high proximity ). Use their known values to fill in missing ones.\n",
    "- Outlier Detection Using Proximity Matrix : Rarely appear in same leaf as others → low proximity to everyone.\n",
    "\n",
    "$$\n",
    "\\text{Outlier Score }(i) = \\frac{1}{\\sum_j \\text{Proximity(i,j)}^2}\n",
    "$$\n",
    "\n",
    "- Large Score → Isolated → Likely outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf6cfc0-7916-4de8-a6b1-250a69073f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Score: 0.917\n",
      "Test Score: 1.000\n",
      "Proximity Matrix (first 5 samples):\n",
      "[[1.   0.79 0.   1.   1.  ]\n",
      " [0.79 1.   0.   0.79 0.79]\n",
      " [0.   0.   1.   0.   0.  ]\n",
      " [1.   0.79 0.   1.   1.  ]\n",
      " [1.   0.79 0.   1.   1.  ]]\n",
      "Outlier Scores (first 10 samples):\n",
      "[0.02582685 0.03834812 0.03830229 0.02582685 0.02582685 0.07265647\n",
      " 0.04187447 0.02582685 0.02582685 0.02582685]\n",
      "Top 3 Outliers indices: [68 59 62]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# -----------------------------\n",
    "# Load example dataset\n",
    "# -----------------------------\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data (just for demonstration)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# Random Forest with OOB\n",
    "# -----------------------------\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_features='sqrt',\n",
    "    oob_score=True,   # enable Out-Of-Bag score\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"OOB Score: {rf.oob_score_:.3f}\")\n",
    "print(f\"Test Score: {rf.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Proximity Matrix Calculation\n",
    "# -----------------------------\n",
    "# Idea: For each tree, get leaf indices. Count how often samples i & j share same leaf.\n",
    "n_samples = X_train.shape[0]\n",
    "proximity = np.zeros((n_samples, n_samples))\n",
    "\n",
    "# For each tree\n",
    "for tree in rf.estimators_:\n",
    "    # Leaf indices for each training sample\n",
    "    leaf_indices = tree.apply(X_train)\n",
    "    # Compare every pair of samples\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i, n_samples):\n",
    "            if leaf_indices[i] == leaf_indices[j]:\n",
    "                proximity[i, j] += 1\n",
    "                if i != j:\n",
    "                    proximity[j, i] += 1  # symmetric\n",
    "\n",
    "# Normalize by number of trees\n",
    "proximity /= len(rf.estimators_)\n",
    "print(\"Proximity Matrix (first 5 samples):\")\n",
    "print(proximity[:5, :5])\n",
    "\n",
    "# -----------------------------\n",
    "# Outlier Detection Using Proximity\n",
    "# -----------------------------\n",
    "# Outlier Score: 1 / sum_j Proximity(i,j)^2\n",
    "outlier_scores = 1 / np.sum(proximity**2, axis=1)\n",
    "print(\"Outlier Scores (first 10 samples):\")\n",
    "print(outlier_scores[:10])\n",
    "\n",
    "# Identify top outliers\n",
    "top_outliers = np.argsort(outlier_scores)[-3:]\n",
    "print(\"Top 3 Outliers indices:\", top_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8a66f-a79c-4363-8d6c-c78472907328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

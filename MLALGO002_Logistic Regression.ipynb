{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2026a00-e487-4c05-8b4e-daf7c3f2dbec",
   "metadata": {},
   "source": [
    "# Model Formulation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b8010-3b9d-4f43-ae81-4340d4cb82d3",
   "metadata": {},
   "source": [
    "- Discriminative Probability Classifier\n",
    "- Models the probability of a binary outcome as a smooth function of predictors, constrained between 0 and 1.\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}Y &\\in \\{0,1\\}\\\\P(Y=1) &=p\\\\P(Y=0) &= 1-p\\\\P(Y=y) &= p^y(1-p)^{1-y}\\\\\\end{align*}\n",
    "$$\n",
    "\n",
    "Why is log-odds the natural parameter of Bernoulli distribution?\n",
    "\n",
    "- We start with Bernoulli pdf\n",
    "- Write it in exponential form\n",
    "- The coefficient multiply $y$ must be the natural parameter\n",
    "- so log-odds is not a design choice, it emerges.\n",
    "\n",
    "Consider; $p^y(1-p)^{1-y}$, lets write it in exponential form;\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}p^y(1-p)^{1-y} &= exp(y\\log{p} + (1-y) \\log{1-p}))\\\\&= exp(y\\log{p} + \\log(1-p) - y\\log{1-p})\\\\&= exp(y\\log{\\frac{p}{1-p}}+ \\log{1-p})\\\\\\end{align*}\n",
    "$$\n",
    "\n",
    "Canonical exponential form is as follows; \n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}f(y|\\theta) = exp(y\\theta - A(\\theta))\\end{align*}\n",
    "$$\n",
    "\n",
    "So, based on our exponential form of Bernoulli, we have ; \n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}\\theta &= \\log{\\frac{p}{1-p}}\\\\-A(\\theta) &= \\log{1-p}\\\\\\end{align*}\n",
    "$$\n",
    "\n",
    "So, $\\log{\\frac{p}{1-p}}$ is a natural parameter.\n",
    "\n",
    "Introducing Covariates : Conditional Bernoulli\n",
    "\n",
    "Once predictors exist, we are no longer modeling p, but ; \n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}p(x) = P(Y=1|X=x)\\end{align*}\n",
    "$$\n",
    "\n",
    "So, the natural parameter becomes conditional; \n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}\\theta(x) = \\log{(\\frac{p(x)}{1-p(x)})}\\end{align*}\n",
    "$$\n",
    "\n",
    "We make one structural assumption; The natural parameter depends linearly on predictors; \n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}\\theta(x) = \\beta_0 + \\beta^Tx\\end{align*}\n",
    "$$\n",
    "\n",
    "- $\\theta \\in (-\\infty,+\\infty)$\n",
    "- Linear structure\n",
    "- Linear natural parameter implies convex log-likelihood\n",
    "\n",
    "When we plug back the theta value into $p(x)$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb231939-aaa2-4aca-b444-95c39732aede",
   "metadata": {},
   "source": [
    "# Linking Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe625f0-59a0-4978-b9d9-3270a51b9a02",
   "metadata": {},
   "source": [
    "- Probit - Normal CDF\n",
    "- Complementary log-log\n",
    "- Cauchit\n",
    "- Only logit is the canonical link for Bernoulli\n",
    "- Canonical link → best statistical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3943124-8562-40e3-abf2-02dc96f6fcbf",
   "metadata": {},
   "source": [
    "# Parameter Estimation using MLE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b90efe-c43c-45a6-b2ee-0b9afea15511",
   "metadata": {},
   "source": [
    "For a single observation $(x_i, y_i)$ with $y_i \\in \\{0,1\\}$, the Bernoulli probability mass function is:\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}P(Y_i = y_i \\mid X_i = x_i)&= p_i^{y_i}(1-p_i)^{1-y_i}\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}p_i = P(Y_i=1 \\mid X_i=x_i)\\end{align*}\n",
    "$$\n",
    "\n",
    "Assuming **conditional independence** of observations given $X$, the likelihood for the full dataset $\\{(x_i,y_i)\\}_{i=1}^n$ is the product of individual likelihoods:\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}L(\\beta)&= \\prod_{i=1}^{n} p_i^{y_i}(1-p_i)^{1-y_i}\\end{align*}\n",
    "$$\n",
    "\n",
    "Here, the probabilities $p_i$ are modeled using the logistic function:\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}p_i&= \\frac{1}{1 + \\exp(-\\eta_i)}\\\\\\eta_i&= \\beta_0 + \\beta^T x_i\\end{align*}\n",
    "$$\n",
    "\n",
    "Since the log function is monotonic, maximising the likelihood is equivalent to maximising the log-likelihood.\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}\\ell(\\beta)&= \\log L(\\beta)\\\\&= \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i)\\log(1-p_i) \\right]\\end{align*}\n",
    "$$\n",
    "\n",
    "This is the **log-likelihood** of logistic regression.\n",
    "In optimisation, it is common to convert a maximisation problem into a minimisation problem. We therefore define the **negative log-likelihood (NLL)**:\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}- \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i)\\log(1-p_i) \\right]\\end{align*}\n",
    "$$\n",
    "\n",
    "The goal of logistic regression is; \n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}\\hat{\\beta}= \\arg\\min_{\\beta} \\; \\mathcal{L}(\\beta)\\end{align*}\n",
    "$$\n",
    "\n",
    "**Logistic Regression chooses $\\beta$ so that the predicted probabilities $\\sigma(\\beta^Tx_i)$ match the observed labels as well as possible.** \n",
    "\n",
    "The negative log-likelihood above is known in machine learning as the **binary cross-entropy loss** (or log loss).\n",
    "For a single observation, the loss is:\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}\\text{BCE}(y_i, p_i)&= - \\left[ y_i \\log(p_i) + (1-y_i)\\log(1-p_i) \\right]\\end{align*}\n",
    "$$\n",
    "\n",
    "For the full dataset; \n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}\\text{BCE}(\\beta)&= \\sum_{i=1}^{n} \\text{BCE}(y_i, p_i)\\end{align*}\n",
    "$$\n",
    "\n",
    "- This loss arises **directly from maximum likelihood estimation**, not as a heuristic.\n",
    "- Thus, minimising binary cross-entropy is equivalent to **maximum likelihood estimation for a Bernoulli model with a logistic link**.\n",
    "\n",
    "Thus, Logistic regression estimates parameters $\\beta$ by:\n",
    "\n",
    "1. Modeling $P(Y=1 \\mid X=x)$ via the logistic (sigmoid) function\n",
    "2. Writing the Bernoulli likelihood for observed labels\n",
    "3. Maximising the log-likelihood (or equivalently, minimising binary cross-entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb5eb0-cbaf-4023-b080-b940f68f12f8",
   "metadata": {},
   "source": [
    "# Optimization Talks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd8044-71cc-4c41-bbc2-cbdc414e3131",
   "metadata": {},
   "source": [
    "Binary cross entropy loss comes. directly from MLE \n",
    "\n",
    "Logistic regression = maximise log-likelihood\n",
    "\n",
    "Equivalently = minimise BCE Loss\n",
    "\n",
    "The algorithm, it starts with random parameters \\beta . The probabilities are computed. There are many algorithms which we can use for iterative optimisation ( logistic regression has no closed form solution)\n",
    "\n",
    "Logistic regression does not have closed form solution. \n",
    "\n",
    "If we take the gradient and set to zero. The equation will be nonlinear in beta. Because sigmoid function contains an exponential. No algebraic manipulation can isolate beta. So, there is no analytic solution to that equation. \n",
    "\n",
    "The sigmoid introduces exponentials of $\\beta$ making the likelihood equations transcendental rather than linear or polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e1209-98e4-4f58-8a72-13f96bb9f4ca",
   "metadata": {},
   "source": [
    "# Canonical Exponential Form\n",
    "\n",
    "A standardised way to write a probability distribution as an exponential of a linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c253062-c17e-442b-b58f-4e39d6927cb9",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52ee72e1-bbf6-48ac-9c83-efb1ef74f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegressionGD:\n",
    "    def __init__(self, x, y, iterations=1000, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        x: np.array of shape (n_samples, n_features)\n",
    "        y: np.array of shape (n_samples, 1) with 0/1 labels\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_iter = iterations\n",
    "        self.lr = learning_rate\n",
    "        self.n_samples, self.n_features = x.shape\n",
    "\n",
    "        # weights (column vector) and bias\n",
    "        self.w = np.zeros((self.n_features, 1))\n",
    "        self.b = 0\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def train(self, verbose=False):\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            # linear model\n",
    "            z = self.x.dot(self.w) + self.b\n",
    "            y_pred = self.sigmoid(z)\n",
    "\n",
    "            # residuals / error\n",
    "            residuals = y_pred - self.y  # shape (n_samples,1)\n",
    "\n",
    "            # gradients\n",
    "            dw = (1 / self.n_samples) * self.x.T.dot(residuals)\n",
    "            db = np.mean(residuals)\n",
    "\n",
    "            # update weights\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "\n",
    "            if verbose and (i % 100 == 0 or i == 1):\n",
    "                # cross-entropy loss\n",
    "                eps = 1e-15  # to avoid log(0)\n",
    "                loss = -np.mean(self.y * np.log(y_pred + eps) + (1 - self.y) * np.log(1 - y_pred + eps))\n",
    "                print(f\"Iteration {i}: Loss = {loss:.4f}, Weights = {self.w.flatten()}, Bias = {self.b:.4f}\")\n",
    "\n",
    "    def predict_proba(self, x_new):\n",
    "        z = x_new.dot(self.w) + self.b\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "    def predict(self, x_new, threshold=0.5):\n",
    "        prob = self.predict_proba(x_new)\n",
    "        return (prob >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79ef4d07-5131-4edf-974f-c4b680f7109a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 0.6931, Weights = [0.03993587 0.02240787], Bias = 0.0400\n",
      "Iteration 100: Loss = 0.1921, Weights = [1.13718    1.01517886], Bias = 1.0452\n",
      "Iteration 200: Loss = 0.1450, Weights = [1.45763373 1.60411047], Bias = 1.1500\n",
      "Iteration 300: Loss = 0.1177, Weights = [1.6975981  2.06600644], Bias = 1.1682\n",
      "Iteration 400: Loss = 0.0990, Weights = [1.90727415 2.44381287], Bias = 1.1720\n",
      "Iteration 500: Loss = 0.0852, Weights = [2.09610572 2.76188257], Bias = 1.1742\n",
      "Iteration 600: Loss = 0.0747, Weights = [2.26742613 3.03595227], Bias = 1.1768\n",
      "Iteration 700: Loss = 0.0665, Weights = [2.4235189  3.27650733], Bias = 1.1802\n",
      "Iteration 800: Loss = 0.0598, Weights = [2.56635126 3.49076495], Bias = 1.1841\n",
      "Iteration 900: Loss = 0.0544, Weights = [2.69764648 3.68386545], Bias = 1.1886\n",
      "Iteration 1000: Loss = 0.0498, Weights = [2.81889003 3.85958716], Bias = 1.1933\n",
      "Predictions:\n",
      " [[1]\n",
      " [1]\n",
      " [1]]\n",
      "Probabilities:\n",
      " [[0.76732981]\n",
      " [0.99961879]\n",
      " [0.99774777]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Sample data (10 samples, 2 features)\n",
    "X = np.random.randn(10, 2)\n",
    "y = (X[:,0] + X[:,1] > 0).astype(int).reshape(-1,1)  # simple linear separable labels\n",
    "\n",
    "# Train logistic regression\n",
    "model = LogisticRegressionGD(X, y, iterations=1000, learning_rate=0.1)\n",
    "model.train(verbose=True)\n",
    "\n",
    "# Predict\n",
    "X_new = np.array([[0,0],[1,1],[-1,2]])\n",
    "predictions = model.predict(X_new)\n",
    "probs = model.predict_proba(X_new)\n",
    "\n",
    "print(\"Predictions:\\n\", predictions)\n",
    "print(\"Probabilities:\\n\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49a68ad5-7865-482d-abd3-d818cf82e9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[16.75823926],\n",
       "        [47.44576002]]),\n",
       " np.float64(3.2979292708102528))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(model.w),np.exp(model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adfe8cd-9594-48c5-97d9-6c285824b833",
   "metadata": {},
   "source": [
    "**Interpretations**\n",
    " - $\\beta_1$ - change in log-odds for 1 unit increase in $x_1$\n",
    " - A one -unit increase in $x_1$ multiplies the odds of $y=1$ by 16.75 , holding other variables constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be358a2e-ad6b-469b-9ca8-f1802eb3632e",
   "metadata": {},
   "source": [
    "# Key-Logistic Assumptions\n",
    " - Logit is linear in parameters \n",
    " $$\n",
    " \\Large \\log\\left(\\frac{p}{1-p}\\right) = X\\beta\n",
    " $$ \n",
    " - Observations are independent\n",
    " - No severe multicollinearity\n",
    " - No perfect separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1852615-a587-4b35-a9bd-29682e526aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.485725\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   80\n",
      "Model:                          Logit   Df Residuals:                       77\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Sun, 01 Feb 2026   Pseudo R-squ.:                  0.2992\n",
      "Time:                        16:38:12   Log-Likelihood:                -38.858\n",
      "converged:                       True   LL-Null:                       -55.452\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.214e-08\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0211      0.281     -0.075      0.940      -0.572       0.530\n",
      "x1             0.6451      0.321      2.012      0.044       0.017       1.273\n",
      "x2             1.7555      0.411      4.266      0.000       0.949       2.562\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples\n",
    "n = 100\n",
    "\n",
    "# Generate 2 features\n",
    "X1 = np.random.randn(n)\n",
    "X2 = np.random.randn(n)\n",
    "\n",
    "# Linear combination + noise\n",
    "linear_comb = 0.5*X1 + 1.2*X2 - 0.3\n",
    "\n",
    "# Apply sigmoid to get probabilities\n",
    "prob = 1 / (1 + np.exp(-linear_comb))\n",
    "\n",
    "# Generate binary labels\n",
    "y = np.random.binomial(1, prob, size=n)\n",
    "\n",
    "# Combine features\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Split into train/test (simple 80/20 split)\n",
    "train_size = int(0.8 * n)\n",
    "x_train, x_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Add constant for intercept\n",
    "X_sm = sm.add_constant(x_train)\n",
    "\n",
    "# Fit logistic regression\n",
    "logit_model = sm.Logit(y_train, X_sm)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Print summary\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03b1fdf7-0de3-4bc2-a14b-bee129b6b7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02106762  0.64510975  1.75547306]\n",
      "[[-0.57220158  0.53006635]\n",
      " [ 0.01676804  1.27345147]\n",
      " [ 0.94898477  2.56196135]]\n",
      "[0.97915276 1.90619622 5.78618433]\n",
      "[[ 0.56428176  1.69904504]\n",
      " [ 1.01690941  3.57316396]\n",
      " [ 2.58308591 12.96121394]]\n"
     ]
    }
   ],
   "source": [
    "params = result.params\n",
    "conf = result.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(params)\n",
    "print(conf)\n",
    "print(odds_ratios)\n",
    "print(conf_odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37968068-bf0b-4efc-9d8c-49ef4c6adae9",
   "metadata": {},
   "source": [
    "# Odds, log -odds, odds ratio - Interpretations \n",
    "\n",
    "$$\n",
    "\\Large odds = \\frac{p}{1-p}\n",
    "$$ \n",
    "* Odds gives - success ( p ) is how many times as likely as failure.\n",
    "* This value ranges from 0 to infinity, can't model linearly with unrestricted Xβ\n",
    "* When we take the log of odds, this ranges from -infinity to + inifity.\n",
    "* So +log odds means probability >0.5 and -ve log odds means probability <0.5\n",
    "* In logistic regression we assume the following; The log odds has has a linear parameter structure with the predictors.This value can be modelled linearly with unrestricted Xβ\n",
    "\n",
    "$$ \n",
    "\\Large \\log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 x_1 + ...\n",
    "$$\n",
    "\n",
    "This ensures that we can get the probability between 0 and 1 ( using sigmoid )\n",
    "\n",
    "$$\n",
    "\\Large \\log(\\frac{p}{1-p}) = -1.05 + 1.95 x_1 -0.98 x_2\n",
    "$$\n",
    "\n",
    "* If Odds Ratios of the parameters are : 0.35, 7.0 and 0.37 respectively\n",
    "* One unit increase in $x_1$ multiplies the odds of y=1 by 7\n",
    "* One unit increase in $x_2$ reduces odds by 63% ( 1-0.37).\n",
    "* 0.35 is the odds of y=1 at $x_1 = x_2 = 0 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07848f2d-84ee-4a32-9e98-80f97407c6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02322445]), array([[0.54384023, 1.5090275 ]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sk_model = LogisticRegression(fit_intercept=True, solver=\"lbfgs\")\n",
    "sk_model.fit(x_train, y_train)\n",
    "\n",
    "sk_model.intercept_, sk_model.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e7e2d7-e479-400f-a8cd-611fa6918eaa",
   "metadata": {},
   "source": [
    "# Overall Model Significance - Likelihood Ratio Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f6503dd-93ed-4142-825d-a3a6ef55e812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(33.1876282883203), np.float64(6.214383967598268e-08))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "X_sm = sm.add_constant(x_train)\n",
    "\n",
    "# Null model (intercept only)\n",
    "model_null = sm.Logit(y_train, np.ones((len(y_train), 1))).fit(disp=False)\n",
    "\n",
    "# Full model\n",
    "model_full = sm.Logit(y_train, X_sm).fit(disp=False)\n",
    "\n",
    "LR_stat = 2 * (model_full.llf - model_null.llf)\n",
    "df = X_sm.shape[1] - 1\n",
    "p_value = stats.chi2.sf(LR_stat, df)\n",
    "\n",
    "LR_stat, p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeccf5c-8662-45c6-be4e-6025bd084c53",
   "metadata": {},
   "source": [
    "# Wald Test - Single Coefficient Significance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee48f6db-d552-4aba-ab7d-6e4b3aa0f10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.012267921478266), np.float64(0.04419170202581291))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "beta_hat = result.params\n",
    "se = result.bse\n",
    "\n",
    "# Wald test for coefficient j (example: x1 → index 1)\n",
    "j = 1\n",
    "\n",
    "z_stat = beta_hat[j] / se[j]\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "\n",
    "z_stat, p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a0520-ed4e-4dcd-aea0-955ae1986caf",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129c913-eb31-4561-baed-1e9cd75298a9",
   "metadata": {},
   "source": [
    "    \n",
    "Aspect                             | Linear Regression                 | Logistic Regression                           |\n",
    "| :-------------------------------- | :-------------------------------- | :-------------------------------------------- |\n",
    "| **Linear assumption**              | (y) linear in (X)                 | **Log-odds** linear in (X)                    |\n",
    "| **Error distribution**             | Normal                            | Binomial variance (p(1-p))                    |\n",
    "| **Variance of response**           | Constant ((\\sigma^2))             | Mean-dependent                                |\n",
    "| **Estimation method**              | Least Squares                     | Maximum Likelihood                            |\n",
    "| **Overall model test**             | ANOVA F-test                      | Likelihood Ratio Test (LRT)                   |\n",
    "| **Individual parameter test**      | t-test                            | Wald test                                     |\n",
    "| **Alternative tests**              | Partial F-test                    | Wald / LRT / Score                            |\n",
    "| **Model comparison**               | F-test                            | Likelihood ratio                              |\n",
    "| **Test statistic distribution**    | F, t                              | $( \\chi^2 )$, Normal (asymptotic)              |\n",
    "| **Confidence intervals**           | Exact (normality)                 | Asymptotic (Wald / profile)                  |\n",
    "| **Interpretation of coefficients** | Change in mean (y)                | Change in **log-odds**                        |\n",
    "| **Natural effect scale**           | Units of (y)                      | Odds ratios                                   |\n",
    "| **Exponentiated coefficients**     | Meaningless                        | Odds ratios                                   |\n",
    "| **Residuals**                      | Raw, standardized                 | Deviance, Pearson                             |\n",
    "| **Residual patterns**              | Homoscedastic                     | Mean–variance linked                          |\n",
    "| **Pseudo-R²**                      | Not needed                         | McFadden, Cox–Snell                           |\n",
    "| **Influence measures**             | Cook’s distance                   | Leverage, ΔDeviance                           |\n",
    "| **Perfect separation**             | Not an issue                      | Can break MLE                                 |\n",
    "| **Multicollinearity**              | Inflates SEs                      | Inflates SEs                                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecd0d2-f960-47ae-8501-25a9944ead73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

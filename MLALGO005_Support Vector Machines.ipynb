{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f852860-df06-4bda-9856-3f01d7b2552e",
   "metadata": {},
   "source": [
    "# Algorithm & Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004ae0b-7e08-4d64-a590-196c8be8de4e",
   "metadata": {},
   "source": [
    "Support Vector Machine is a supervised learning algorithm for classification, regression and outlier detection. SVM finds the best decision boundary that maximizes the margin between classes. \n",
    "\n",
    "- Maximizing margin → better generalization\n",
    "- Uses convex optimization → global optimum guaranteed\n",
    "- Can model non-linear decision boundaries using kernels\n",
    "\n",
    "$$\n",
    "\\Large \\{(x_i, y_i)\\}_{i=1}^n, \\quad x_i \\in \\mathbb{R}^d,\\; y_i \\in \\{-1, +1\\}\n",
    "$$\n",
    "\n",
    "We begin with the simplest possible classifier; a hyperplane. \n",
    "\n",
    "$$\n",
    "\\Large w.x + b = 0\n",
    "$$\n",
    "\n",
    "where w is a d dimensional normal vector and b is the bias ; the classification rule is as follows ; \n",
    "\n",
    "$$\n",
    "\\LARGE \\hat{y} =\\begin{cases}+1, & \\text{if } w \\cdot x + b \\ge 0 \\\\-1, & \\text{if } w \\cdot x + b < 0\\end{cases}\n",
    "$$\n",
    "\n",
    "If the data is linearly separable, there exist infinitely many hyperplanes that correctly classify the data. So the question is Which hyperplane should we choose for best generalization?. \n",
    "\n",
    "Support vector machines answer this by adopting the maximum margin principle;  Among all separating hyperplanes, choose the one that maximises the distance to the nearest data points from both classes. This distance is called the margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3607ef87-0f8e-409b-a4e7-c07f20211bb5",
   "metadata": {},
   "source": [
    "# Objective & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a78f53-fb2a-4195-82b4-ff9a87a5c02b",
   "metadata": {},
   "source": [
    "To formalize the margin, we fix the scale of $(w,b)$ and define two parallel hyperplanes; \n",
    "\n",
    "Positive Margin Hyperplane; $w.x+b=+1$ and Negative Margin Hyperplane; $w.x+b=-1$. These hyperplanes are parallel to the decision boundary and pass through the closest points of each class( support vectors).\n",
    "\n",
    "All training points must lie outside or on the margin hyperplanes; ie.\n",
    "\n",
    " $\\Large y_i(w.x_i+b)≥1  \\text{ for all i}$\n",
    "\n",
    "The above inequality ensures correct classification and enforces a minimum margin. \n",
    "\n",
    "Distance between two hyperplanes is given by ; \n",
    "\n",
    "$$\n",
    "\\Large \\text{Margin width} = \\frac{2}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "So our objective is to maximise this margin or equivalently minimizing $\\|w\\|$\n",
    "\n",
    "Final Primal Optimization Problem ( Hard Margin SVM )\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,\\, b} \\quad & \\frac{1}{2}\\|w\\|^2 \\\\\\text{subject to} \\quad & y_i \\bigl(w \\cdot x_i + b\\bigr) \\ge 1,\\quad i = 1, \\dots, n\\end{aligned}\n",
    "$$\n",
    "\n",
    "We write the constraints in standard form ; \n",
    "\n",
    "$$\n",
    "g_i(w,b) = 1 - y_i (w \\cdot x_i + b) \\le 0\n",
    "$$\n",
    "\n",
    "We introduce Lagrange Multipliers; one multiplier per constraint. Each multiplier enforces one constraint. \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}\\|w\\|^2+\\sum_{i=1}^n\\alpha_i \\left(1 - y_i (w \\cdot x_i + b)\\right)\n",
    "$$\n",
    "\n",
    "We take partial derivate with respect to w; and set it zero ; \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w}=w - \\sum_{i=1}^n \\alpha_i y_i x_i\n",
    "$$\n",
    "\n",
    "We take the partial derivate with respect to b\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b}=- \\sum_{i=1}^n \\alpha_i y_i\n",
    "$$\n",
    "\n",
    "Up to now we assumed that Data is perfectly separable; but in practice data is noisy and classes overlap . perfect separation may not exist. So the original constraint $y_i(w.x_i+b)≥1$ may be impossible to satisfy. Slack variables allow controlled violations. \n",
    "\n",
    "$$\n",
    "y_i \\bigl(w \\cdot x_i + b\\bigr) \\ge 1 - \\xi_i\n",
    "$$\n",
    "\n",
    "- $0≤\\xi_i<1$ - inside margin, correct side;\n",
    "- $\\xi_i=0$ - correct , outside margin\n",
    "- $\\xi_i=1$ - on decision boundary\n",
    "- $\\xi_i>1$ - misclassified\n",
    "\n",
    "Following is the updated Optimization Objective ( Soft-Margin SVM) \n",
    "\n",
    "$$\n",
    "\\min_{w,\\, b,\\, \\xi}\\quad\\frac{1}{2}\\|w\\|^2+C \\sum_{i=1}^n \\xi_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_i(w \\cdot x_i + b\\bigr) \\ge 1 - \\xi_i,\\quad\\xi_i \\ge 0\n",
    "$$\n",
    "\n",
    "C controls penalty strength; \n",
    "\n",
    "Once we add slack variables, we have two type of constraints in the Lagrange function.\n",
    "\n",
    " \n",
    "\n",
    "$$\n",
    "\\Large \\mathcal{L}=\\frac{1}{2}\\|w\\|^2+C \\sum_{i=1}^n \\xi_i+\\sum_{i=1}^n \\alpha_i \\bigl(1 - \\xi_i - y_i (w \\cdot x_i + b)\\bigr)-\\sum_{i=1}^n \\mu_i \\xi_i\n",
    "$$\n",
    "\n",
    "when we take the derivates ; we get a bound; \n",
    "\n",
    "$$\n",
    "0<=\\alpha_i<=C\n",
    "$$\n",
    "\n",
    "The slack penalty C limits how much violation we are willing to tolerate. No point should dominate the solution arbitrarily. So , SVM caps the influence of any single point. \n",
    "\n",
    "- $\\alpha_i=0$, point is irrelevant\n",
    "- $0<\\alpha_i<C$ , point lies on margin\n",
    "- $\\alpha_i = C$, point violates margin or is misclassified.\n",
    "\n",
    "After solving the optimization problem, we plug in the optimal parameters; \n",
    "\n",
    "we substitute w back into model ; \n",
    "\n",
    "$$\n",
    "f(x) = w \\cdot x + b=\\left( \\sum_{i=1}^n \\alpha_i y_i x_i \\right) \\cdot x + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(x)=\\sum_{i=1}^n \\alpha_i y_i (x_i \\cdot x) + b\n",
    "$$\n",
    "\n",
    "Following is the kernel generalisation of the objective function ;\n",
    "\n",
    "$$\n",
    "f(x)=\\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92cb9f0-8846-48f0-800f-7d0444eb648b",
   "metadata": {},
   "source": [
    "# Kernel Trick\n",
    "\n",
    "In many problems, the data may not be linearly separable in the input space. So we try to map data into a higher dimensional space; \n",
    "\n",
    "$$\n",
    "\\Large \\phi: \\mathbb{R}^d \\rightarrow \\mathcal{H}\n",
    "$$\n",
    "\n",
    "Then the decision function becomes ; \n",
    "\n",
    "$$\n",
    "\\Large f(x) = \\sum_{i=1}^n \\alpha_i y_i (\\phi(x_i) \\cdot \\phi(x)) + b\n",
    "$$\n",
    "\n",
    "But computing $\\phi(x)$ explicitly may be very expensive, infinite dimensional or may be impossible in practice. So if we can find a function K such that ;\n",
    "\n",
    "$$\n",
    "\\Large K(x_i, x) = \\phi(x_i) \\cdot \\phi(x)\n",
    "$$\n",
    "\n",
    "Then we dont not need to compute $\\phi(x)$; Now, the final Kernelized Decision Function becomes ; \n",
    "\n",
    "$$\n",
    "\\Large f(x) =\\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\n",
    "$$\n",
    "\n",
    "The Gaussian ( RBF) kernel corresponds to an infinite - dimensional feature space ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b7465-0a73-4a59-93d0-30b47f50e7c4",
   "metadata": {},
   "source": [
    "# Simple Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1249ecb-2cf7-4896-a68c-85a89e29c9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [-0.52568088  0.8202978 ]\n",
      "Bias: 0.5000000000000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Toy linearly separable dataset\n",
    "X = np.array([[2, 2], [4, 4], [4, 0], [6, 2]])\n",
    "y = np.array([1, 1, -1, -1])  # labels {-1, +1}\n",
    "\n",
    "# Initialize weights and bias\n",
    "w = np.zeros(2)\n",
    "b = 0.0\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Gradient descent on primal SVM (simplified)\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X)):\n",
    "        if y[i] * (np.dot(w, X[i]) + b) < 1:\n",
    "            # Misclassified or inside margin\n",
    "            w += lr * (y[i] * X[i] - 2 * 0.01 * w)  # 0.01 = regularization\n",
    "            b += lr * y[i]\n",
    "        else:\n",
    "            # Correct and outside margin\n",
    "            w -= lr * 2 * 0.01 * w  # only regularization\n",
    "\n",
    "print(\"Weights:\", w)\n",
    "print(\"Bias:\", b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299bd32d-f849-4aeb-a37f-7543e0bdb90a",
   "metadata": {},
   "source": [
    "# Soft Margin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0476b3b8-d08e-46b1-8dfe-47b27721f6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (w): [[-0.5       1.249856]]\n",
      "Bias (b): [-0.499808]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Example dataset\n",
    "X = np.array([[2, 2], [4, 4], [4, 0], [6, 2], [5, 5], [1, 0]])\n",
    "y = np.array([1, 1, -1, -1, 1, -1])\n",
    "\n",
    "# Soft-margin linear SVM\n",
    "C = 1.0\n",
    "clf = SVC(kernel='linear', C=C)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"Weights (w):\", clf.coef_)\n",
    "print(\"Bias (b):\", clf.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59819058-b62d-4cbd-a4ec-d8e830889e15",
   "metadata": {},
   "source": [
    "# Non-linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7602ad-4b83-4625-a71e-cd97875b3a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vectors:\n",
      " [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Non-linearly separable data\n",
    "X = np.array([[0, 0], [1, 1], [0, 1], [1, 0]])\n",
    "y = np.array([1, 1, -1, -1])\n",
    "\n",
    "# RBF kernel SVM\n",
    "clf_rbf = SVC(kernel='rbf', C=1.0, gamma=1.0)\n",
    "clf_rbf.fit(X, y)\n",
    "\n",
    "print(\"Support vectors:\\n\", clf_rbf.support_vectors_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08edee-af57-49cf-b786-92b26c50e880",
   "metadata": {},
   "source": [
    "# rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789697be-93fc-462f-87c2-6579dcfa59da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF Kernel matrix:\n",
      " [[1.         0.13533528 0.36787944 0.36787944]\n",
      " [0.13533528 1.         0.36787944 0.36787944]\n",
      " [0.36787944 0.36787944 1.         0.13533528]\n",
      " [0.36787944 0.36787944 0.13533528 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def rbf_kernel(X1, X2, gamma=1.0):\n",
    "    K = np.zeros((X1.shape[0], X2.shape[0]))\n",
    "    for i in range(X1.shape[0]):\n",
    "        for j in range(X2.shape[0]):\n",
    "            K[i,j] = np.exp(-gamma * np.linalg.norm(X1[i]-X2[j])**2)\n",
    "    return K\n",
    "\n",
    "K = rbf_kernel(X, X, gamma=1.0)\n",
    "print(\"RBF Kernel matrix:\\n\", K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e8fb1-1ee5-497d-80fd-15a16f6f57ac",
   "metadata": {},
   "source": [
    "# Kernel : Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669847e4-de7d-4870-8022-414ec02df143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(121.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define an explicit feature mapping ϕ(x)\n",
    "# Compute dot product in feature space\n",
    "# Compute the kernel directly in input space\n",
    "# Show they are exactly the same → so mapping is unnecessary\n",
    "# \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2])\n",
    "z = np.array([3, 4])\n",
    "\n",
    "def phi(x):\n",
    "    return np.array([\n",
    "        x[0] ** 2,\n",
    "        np.sqrt(2) * x[0] * x[1],\n",
    "        x[1] ** 2\n",
    "    ])\n",
    "phi_x = phi(x)\n",
    "phi_z = phi(z)\n",
    "\n",
    "dot_feature_space = np.dot(phi_x, phi_z)\n",
    "dot_feature_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b2f7a7-b236-4b21-b72f-62f8f7c2cbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(121)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_value = (np.dot(x, z)) ** 2\n",
    "kernel_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d6183b-cff8-49c4-910c-a83b9ba42fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(dot_feature_space, kernel_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd327ee1-8bcf-4476-b4f1-943c107d8d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b369b8-b6ca-4d30-a7ad-b877e1909b4c",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3336f3c1-3c51-4975-84d5-c6a5b0fad0e7",
   "metadata": {},
   "source": [
    "Given, some features $X=(x_1,x_2,…,x_n)$ and class $Y \\in \\Set{C_1,C_2,…,C_k}$, we want to compute $P(Y=C_k|X)$ and choose the class with the highest probability. \n",
    "\n",
    "- Probability Classifier\n",
    "- Generative : like GMM\n",
    "\n",
    "Everything starts from Bayes’ theorem; \n",
    "\n",
    "$$\n",
    "\\Large \\begin{align*}P(Y|X) = \\frac{P(X|Y) P(Y)}{P(X)} \\end{align*}\n",
    "$$\n",
    "\n",
    "- $P(Y)$ → Prior ( how common the class is)\n",
    "- $P(X|Y)$ → Likelihood ( how likely the features are given the class.\n",
    "- $P(X)$ → Evidence ( normalization constant). This is same for every class because it is compute by summing over all classes.\n",
    "\n",
    "$$\n",
    "\\begin{align*}P(Y|X) \\propto P(X|Y) P(Y) \\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*} \\hat{Y} = \\text{ArgMin}_Y P(X|Y) P(Y)\\end{align*}\n",
    "$$\n",
    "\n",
    "Computing the likelihood grows exponentially with number of features; $P(X|Y) = P(x_1,x_2,…,x_n|Y)$.\n",
    "\n",
    "Here we are assuming that all features are conditionally independent given the class; Mathematically; \n",
    "\n",
    "$$\n",
    "\\begin{align*}P(x_1,x_2,...,x_n|Y) = \\prod_{i=1}^{n} P(x_i|Y)\\end{align*}\n",
    "$$\n",
    "\n",
    "This assumption is almost always false in real data. Words in a sentence are correlated. Pixels in an image are correlated. Medical symptoms are correlated. We call that this assumption is Naive because it makes an unrealistically strong independence assumption. \n",
    "\n",
    "$$\n",
    "\\begin{align*}P(Y|x_1,x_2,...,x_n)\\propto P(Y) \\prod_{i=1}^{n} P(x_i|Y)\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{Y}=\\arg\\max_{Y}\\left[\\log P(Y)+\\sum_{i=1}^{n} \\log P(x_i \\mid Y)\\right]\n",
    "$$\n",
    "\n",
    "How we estimate the probabilities?\n",
    "\n",
    "Priors can be estimated using the following formula; \n",
    "\n",
    "$$\n",
    "\\begin{align*}P(Y=C_k)=\\frac{\\text{Number of Samples in the Class } C_k}{\\text{Total Number of Samples}}\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87969226-b331-45a9-9acd-c5e7cb2fdb8c",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes ( continuous features)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Large x_i \\mid Y = \\Large C_k  &\\Large\\sim \\mathcal{N}\\!\\left(\\mu_{ik}, \\sigma_{ik}^2\\right)\\\\\n",
    "\\Large P(x_i \\mid Y = C_k)&=\\Large \\frac{1}{\\sqrt{2\\pi\\sigma_{ik}^2}}\\exp\\!\\left(-\\frac{(x_i - \\mu_{ik})^2}{2\\sigma_{ik}^2}\\right)\\end{align*}\n",
    "$$\n",
    "\n",
    "In Gaussian Naive Bayes, we assume a class-conditional Gaussian distribution, write down the likelihood of the observed data, take its log, and maximise it with respect to the parameters, which yields the same mean and MLE sample variance of each feature within each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce2d20d-d9c9-4876-9406-aa15fad7960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Toy dataset: features are continuous\n",
    "X_train = np.array([\n",
    "    [5.1, 3.5],\n",
    "    [4.9, 3.0],\n",
    "    [6.2, 3.4],\n",
    "    [5.9, 3.0],\n",
    "    [7.0, 3.2],\n",
    "    [6.4, 3.2]\n",
    "])\n",
    "\n",
    "y_train = np.array([0, 0, 1, 1, 2, 2])  # 3 classes\n",
    "\n",
    "# Query point\n",
    "x_query = np.array([6.0, 3.0])\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Compute class priors\n",
    "# -------------------------------\n",
    "classes = np.unique(y_train)\n",
    "priors = {c: np.mean(y_train == c) for c in classes}\n",
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe0ec93-b0fe-4422-b485-b2f169f7b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Step 2: Compute class-wise mean & variance (MLE)\n",
    "# -------------------------------\n",
    "mean_var = {}\n",
    "for c in classes:\n",
    "    X_c = X_train[y_train == c]\n",
    "    mean_var[c] = (X_c.mean(axis=0), X_c.var(axis=0))\n",
    "\n",
    "mean_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156f050-6b08-4d43-ad8f-b38dbe82b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Step 3: Gaussian likelihood\n",
    "# -------------------------------\n",
    "def gaussian_likelihood(x, mean, var):\n",
    "    eps = 1e-8  # numerical stability\n",
    "    coeff = 1 / np.sqrt(2 * np.pi * var + eps)\n",
    "    exponent = np.exp(- (x - mean) ** 2 / (2 * var + eps))\n",
    "    return coeff * exponent\n",
    "\n",
    "# Compute posterior for each class\n",
    "posteriors = {}\n",
    "for c in classes:\n",
    "    mean, var = mean_var[c]\n",
    "    likelihood = np.prod(gaussian_likelihood(x_query, mean, var))\n",
    "    posteriors[c] = priors[c] * likelihood\n",
    "\n",
    "# Predicted class\n",
    "pred_class = max(posteriors, key=posteriors.get)\n",
    "pred_class, posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ae887-ff18-4faa-b18e-6c6dd50f83bc",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes ( Binary features)\n",
    "\n",
    "$$\n",
    "\\Large \\begin{align*}P(x_i \\mid Y)&=p_i^{x_i}\\,(1 - p_i)^{1 - x_i}\\end{align*}\n",
    "$$\n",
    "\n",
    "Naive Bayes naturally supports mixed feature types - you just use a different likelihood model for each feature, and multiply them all together.  - independent likelihoods, each with its own distribution.\n",
    "\n",
    "Use Bernoulli Naive Bayes when features represent binary presence/absence, and repeated occurrences carry no additional meaning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9847d59-fed7-4408-8883-e40859d4bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    [1, 1, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a28e4dd-7045-4dd9-a11d-73269fe6d2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.25, 3.25, 2.25])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_probability = data.sum(axis=0)+1 / data.shape[0]# feature wise sum  \n",
    "feature_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cc46729-e5c5-494c-b5cf-d6f8872b856a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_query_bin = np.array([1, 0, 1])\n",
    "x_query_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efd321f6-6e58-4c78-b27f-bdd995267d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.25, 0.  , 2.25])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_probability*x_query_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75a284a9-acff-48ff-9d69-f963f01c1af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0),\n",
       " {np.int64(0): np.float64(0.140625), np.int64(1): np.float64(0.015625)})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Toy binary dataset\n",
    "X_train_bin = np.array([\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "y_train_bin = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Query point\n",
    "x_query_bin = np.array([1, 0, 1])\n",
    "\n",
    "# Step 1: Priors\n",
    "classes = np.unique(y_train_bin)\n",
    "priors = {c: np.mean(y_train_bin == c) for c in classes}\n",
    "\n",
    "# Step 2: Likelihood (Bernoulli)\n",
    "likelihoods = {}\n",
    "for c in classes:\n",
    "    X_c = X_train_bin[y_train_bin == c]\n",
    "    # Feature-wise probability with Laplace smoothing\n",
    "    feature_prob = (X_c.sum(axis=0) + 1) / (X_c.shape[0] + 2)\n",
    "    # Bernoulli likelihood\n",
    "    likelihood = np.prod(feature_prob ** x_query_bin * (1 - feature_prob) ** (1 - x_query_bin))\n",
    "    likelihoods[c] = priors[c] * likelihood\n",
    "\n",
    "# Prediction\n",
    "pred_class = max(likelihoods, key=likelihoods.get)\n",
    "pred_class, likelihoods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23e18d-a54c-4281-a747-d6ac3ff34e34",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes ( text, counts)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Large P(x_i \\mid Y)&=\\Large \\frac{\\text{count of word } i \\text{ in class } Y + \\alpha}{\\text{total words in class } Y + \\alpha V}\\end{align*}\n",
    "$$\n",
    "\n",
    "To understand more about the multinomial naive bayes, consider the following table ( pet lovers); \n",
    "\n",
    "|  | Cat | Dog | Rabbit | Hamster | Fish | Total |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Class 1  | 18 | 20 | 6 | 4 | 2 | 50 |\n",
    "| Class 2 | 15 | 15 | 10 | 5 | 5 | 50 |\n",
    "| Class 3 | 17 | 18 | 8 | 4 | 3 | 50 |\n",
    "\n",
    "The above table is a hypothetical dataset about how many students prefer a particular animal as a pet. Each row can be viewed as a random vector from a multinomial distribution. For instance, the first row $(18,20,6,4,2)$ can be viewed as a random draw from a multinomial distribution\n",
    "\n",
    " $\\Large M_5(n=50;p_1,p_2,…,p_5)$\n",
    "\n",
    "The second and third row can be viewed as other random draws from the same distribution. \n",
    "\n",
    "The PMF of a multinomial distribution has a simple form. \n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}x = (x_1,x_2,...,x_V)\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Large \\begin{align*}\\sum_i x_i = N\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Large p(X=x) = p(x|Y=C_k) = \\frac{N!}{\\prod_{i}^{}{x_i!}}\\prod_{i}^{} \\theta_{ik}^{x_i}\n",
    "$$\n",
    "\n",
    "Here, $p_i$ is the probability of that category. \n",
    "\n",
    "$N$ = Total words in the document\n",
    "\n",
    "$x_i$ = Count of word $i$\n",
    "\n",
    "\n",
    "The combinatorial term $\\frac{N!}{\\prod_i x_i !}$ counts, how many distinct sequences of length N produce the same bag of words counts x. It depends only on the observed document. It does not depend on the class. \n",
    "\n",
    "From training data, we estimate class-specific word probabilities. The term is identical no matter which class you test against. \n",
    "\n",
    "Even if we drop the combinatorial term, it does not change the ranking of classes. \n",
    "\n",
    "In multinomial Naive Bayes, the combinatorial term reflects the number of ways the observed word counts could be arranged, which depends only on the document and not on the class, so it cancels out when computing the argmax over classes. \n",
    "\n",
    "$$\n",
    "\\Large \\begin{align*}\\hat{y} = \\text{argmax}_k (\\prod_i \\theta_{ik}^{x_i} P(C_k))\\end{align*}\n",
    "$$\n",
    "\n",
    "- log space scoring\n",
    "- Multinomial NB is a linear classifier in count space\n",
    "- Laplace smoothing adds a pseudocount $\\alpha$ to each feature count, ensuring $P(x_i|y)>0$ and preventing the likelihood from collapsing to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28c72dca-5378-40bc-81ad-64e96ecd93b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0),\n",
       " {np.int64(0): np.float64(0.016787377911344853),\n",
       "  np.int64(1): np.float64(0.008206361131980965),\n",
       "  np.int64(2): np.float64(0.01301878287002254)})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toy dataset: counts of 5 animals in 3 classes\n",
    "X_train_counts = np.array([\n",
    "    [18, 20, 6, 4, 2],   # Class 0\n",
    "    [15, 15, 10, 5, 5],  # Class 1\n",
    "    [17, 18, 8, 4, 3]    # Class 2\n",
    "])\n",
    "y_train_counts = np.array([0, 1, 2])\n",
    "\n",
    "# Query document: counts of each animal\n",
    "x_query_counts = np.array([1, 2, 0, 0, 0])\n",
    "\n",
    "# Step 1: Priors\n",
    "classes = np.unique(y_train_counts)\n",
    "priors = {c: np.mean(y_train_counts == c) for c in classes}\n",
    "\n",
    "# Step 2: Class-specific word probabilities with Laplace smoothing\n",
    "alpha = 1  # pseudocount\n",
    "likelihoods = {}\n",
    "for c in classes:\n",
    "    X_c = X_train_counts[y_train_counts == c]\n",
    "    # Total word counts in class\n",
    "    total_count = X_c.sum()\n",
    "    # Probability for each word\n",
    "    probs = (X_c.sum(axis=0) + alpha) / (total_count + alpha * X_c.shape[1])\n",
    "    # Multinomial likelihood (ignore combinatorial term)\n",
    "    likelihood = np.prod(probs ** x_query_counts)\n",
    "    likelihoods[c] = priors[c] * likelihood\n",
    "\n",
    "# Prediction\n",
    "pred_class = max(likelihoods, key=likelihoods.get)\n",
    "pred_class, likelihoods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe440a37-e489-4e18-9c1b-1459e7966e65",
   "metadata": {},
   "source": [
    "# Mixed DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ffefea-b00f-4782-8259-3f5415ea91da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(1),\n",
       " {np.int64(0): np.float64(4.481078130176555e-25),\n",
       "  np.int64(1): np.float64(0.0013051366304736981)})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------------------------------\n",
    "# Toy dataset\n",
    "# -------------------------------\n",
    "# 2 continuous features, 3 binary features, 2 count features\n",
    "X_train = np.array([\n",
    "    [5.1, 3.5, 1, 0, 1, 10, 2],\n",
    "    [4.9, 3.0, 0, 1, 0, 5, 1],\n",
    "    [6.2, 3.4, 1, 1, 0, 8, 3],\n",
    "    [5.9, 3.0, 0, 0, 1, 7, 2],\n",
    "    [7.0, 3.2, 1, 0, 0, 12, 4],\n",
    "    [6.4, 3.2, 0, 1, 1, 9, 3]\n",
    "])\n",
    "\n",
    "# Binary labels (can be multi-class)\n",
    "y_train = np.array([0, 0, 1, 1, 1, 1])\n",
    "\n",
    "# Query point\n",
    "x_query = np.array([6.0, 3.1, 1, 0, 0, 6, 2])\n",
    "\n",
    "# Feature type indices\n",
    "cont_idx = [0, 1]\n",
    "bin_idx = [2, 3, 4]\n",
    "count_idx = [5, 6]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Compute class priors\n",
    "# -------------------------------\n",
    "classes = np.unique(y_train)\n",
    "priors = {c: np.mean(y_train == c) for c in classes}\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Compute likelihoods\n",
    "# -------------------------------\n",
    "\n",
    "def gaussian_likelihood(x, mean, var):\n",
    "    eps = 1e-8\n",
    "    coeff = 1 / np.sqrt(2 * np.pi * var + eps)\n",
    "    exponent = np.exp(- (x - mean) ** 2 / (2 * var + eps))\n",
    "    return coeff * exponent\n",
    "\n",
    "alpha = 1  # Laplace smoothing for Bernoulli / Multinomial\n",
    "\n",
    "posteriors = {}\n",
    "for c in classes:\n",
    "    X_c = X_train[y_train == c]\n",
    "    \n",
    "    # Continuous features (Gaussian)\n",
    "    mean_c = X_c[:, cont_idx].mean(axis=0)\n",
    "    var_c = X_c[:, cont_idx].var(axis=0)\n",
    "    cont_likelihood = np.prod(gaussian_likelihood(x_query[cont_idx], mean_c, var_c))\n",
    "    \n",
    "    # Binary features (Bernoulli)\n",
    "    X_bin_c = X_c[:, bin_idx]\n",
    "    feature_prob = (X_bin_c.sum(axis=0) + alpha) / (X_bin_c.shape[0] + 2)\n",
    "    x_bin = x_query[bin_idx]\n",
    "    bern_likelihood = np.prod(feature_prob ** x_bin * (1 - feature_prob) ** (1 - x_bin))\n",
    "    \n",
    "    # Count features (Multinomial)\n",
    "    X_count_c = X_c[:, count_idx]\n",
    "    total_count = X_count_c.sum()\n",
    "    probs = (X_count_c.sum(axis=0) + alpha) / (total_count + alpha * len(count_idx))\n",
    "    count_likelihood = np.prod(probs ** x_query[count_idx])\n",
    "    \n",
    "    # Posterior = prior * all likelihoods\n",
    "    posteriors[c] = priors[c] * cont_likelihood * bern_likelihood * count_likelihood\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Prediction\n",
    "# -------------------------------\n",
    "pred_class = max(posteriors, key=posteriors.get)\n",
    "\n",
    "pred_class, posteriors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "412040b8-3be4-4389-9ae0-568461fef9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note : related topic : later reference \n",
    "# kernel density estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b9c1c7-5e8a-4911-87d5-74c03eab87a8",
   "metadata": {},
   "source": [
    "# custom implementation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b09004-9792-41fa-bce1-aeefeec0f432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a0ddce3-331c-45de-95c3-bdd5ed7195ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, X, y, smoothing, feature_types):\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        self.laplace = smoothing\n",
    "        self.feature_types = feature_types\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_priors = {}\n",
    "        self.parameters = {}\n",
    "        self._fit()\n",
    "\n",
    "    def _fit(self):\n",
    "        n_samples = len(self.y)\n",
    "        # Compute class priors\n",
    "        self.class_priors = {c: np.sum(self.y == c)/n_samples for c in self.classes}\n",
    "        \n",
    "        self.parameters = {c: {} for c in self.classes}\n",
    "\n",
    "        for c in self.classes:\n",
    "            X_c = self.X[self.y == c] # taking the respective columns \n",
    "            for idx in range(self.X.shape[1]): # each column/feature\n",
    "                ftype = self.feature_types[idx]\n",
    "                values = X_c[:, idx]\n",
    "\n",
    "                if ftype == 'continuous':\n",
    "                    mean = np.mean(values.astype(float))\n",
    "                    std = np.std(values.astype(float)) + 1e-6  # avoid zero std\n",
    "                    self.parameters[c][idx] = {'type':'continuous', 'mean':mean, 'std':std}\n",
    "\n",
    "                elif ftype == 'level':\n",
    "                    unique_vals, counts = np.unique(values, return_counts=True)\n",
    "                    total = np.sum(counts)\n",
    "                    probs = {v: (counts[i] + self.laplace)/(total + self.laplace*len(unique_vals))\n",
    "                             for i, v in enumerate(unique_vals)}\n",
    "                    self.parameters[c][idx] = {'type':'level', 'probs':probs, 'levels':unique_vals}\n",
    "\n",
    "                elif ftype == 'count':\n",
    "                    # Treat as multinomial/count: probability proportional to value\n",
    "                    total = np.sum(values.astype(float))\n",
    "                    self.parameters[c][idx] = {'type':'count', 'total':total, 'values':values.astype(float)}\n",
    "            \n",
    "\n",
    "    def _gaussian_likelihood(self, x, mean, std):\n",
    "        exponent = -0.5 * ((x - mean)/std)**2\n",
    "        return (1 / (np.sqrt(2*np.pi) * std)) * np.exp(exponent)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.array(X_test)\n",
    "        y_pred = []\n",
    "        for sample in X_test:\n",
    "            log_probs = {}\n",
    "            for c in self.classes:\n",
    "                log_prob = np.log(self.class_priors[c])\n",
    "                for idx, x_i in enumerate(sample):\n",
    "                    param = self.parameters[c][idx]\n",
    "                    ftype = param['type']\n",
    "\n",
    "                    if ftype == 'continuous':\n",
    "                        mean = param['mean']\n",
    "                        std = param['std']\n",
    "                        likelihood = self._gaussian_likelihood(float(x_i), mean, std)\n",
    "                        log_prob += np.log(likelihood + 1e-9)  # avoid log(0)\n",
    "\n",
    "                    elif ftype == 'level':\n",
    "                        probs = param['probs']\n",
    "                        log_prob += np.log(probs.get(x_i, self.laplace*1e-3))  # unseen level handling\n",
    "\n",
    "                    elif ftype == 'count':\n",
    "                        total = param['total']\n",
    "                        prob = (float(x_i) + self.laplace) / (total + self.laplace*len(param['values']))\n",
    "                        log_prob += np.log(prob + 1e-9)\n",
    "\n",
    "                log_probs[c] = log_prob\n",
    "            y_pred.append(max(log_probs, key=log_probs.get))\n",
    "        return np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b0fcb37-3adb-4861-a908-14762da53db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [23.1, 74, 'yes', 'small', 1],\n",
    "    [18.5, 65, 'no', 'medium', 0],\n",
    "    [30.2, 80, 'yes', 'large', 1],\n",
    "    [25.0, 70, 'no', 'small', 0],\n",
    "    [20.1, 68, 'yes', 'medium', 1],\n",
    "    [22.3, 72, 'no', 'large', 0],\n",
    "    [19.8, 66, 'yes', 'small', 1],\n",
    "    [24.5, 75, 'no', 'medium', 0],\n",
    "    [21.0, 69, 'yes', 'large', 1],\n",
    "    [23.9, 73, 'no', 'medium', 0],\n",
    "    [26.1, 78, 'yes', 'small', 1],\n",
    "    [18.9, 64, 'no', 'large', 0],\n",
    "    [29.5, 79, 'yes', 'medium', 1],\n",
    "    [22.0, 71, 'no', 'small', 0],\n",
    "    [20.5, 67, 'yes', 'medium', 1]\n",
    "])\n",
    "y = np.array([0, 2, 0, 1, 0, 1, 0, 1, 2, 1, 1, 2, 0, 1, 1])\n",
    "feature_types = ['continuous','continuous','level','level','level']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2409ddde-a9f8-4c1c-8faf-a9795c33c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(x,y,smoothing=1,feature_types=feature_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "acfe1f6d-2a18-4f80-88f0-ba4d270ecc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb._fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e053074e-da27-466a-81a3-898d49fe82c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0): {0: {'type': 'continuous',\n",
       "   'mean': np.float64(24.54),\n",
       "   'std': np.float64(4.492038399666213)},\n",
       "  1: {'type': 'continuous',\n",
       "   'mean': np.float64(73.4),\n",
       "   'std': np.float64(5.642695391866353)},\n",
       "  2: {'type': 'level',\n",
       "   'probs': {np.str_('yes'): np.float64(1.0)},\n",
       "   'levels': array(['yes'], dtype='<U32')},\n",
       "  3: {'type': 'level',\n",
       "   'probs': {np.str_('large'): np.float64(0.25),\n",
       "    np.str_('medium'): np.float64(0.375),\n",
       "    np.str_('small'): np.float64(0.375)},\n",
       "   'levels': array(['large', 'medium', 'small'], dtype='<U32')},\n",
       "  4: {'type': 'level',\n",
       "   'probs': {np.str_('1'): np.float64(1.0)},\n",
       "   'levels': array(['1'], dtype='<U32')}},\n",
       " np.int64(1): {0: {'type': 'continuous',\n",
       "   'mean': np.float64(23.471428571428568),\n",
       "   'std': np.float64(1.8069038637930912)},\n",
       "  1: {'type': 'continuous',\n",
       "   'mean': np.float64(72.28571428571429),\n",
       "   'std': np.float64(3.282608226593159)},\n",
       "  2: {'type': 'level',\n",
       "   'probs': {np.str_('no'): np.float64(0.6666666666666666),\n",
       "    np.str_('yes'): np.float64(0.3333333333333333)},\n",
       "   'levels': array(['no', 'yes'], dtype='<U32')},\n",
       "  3: {'type': 'level',\n",
       "   'probs': {np.str_('large'): np.float64(0.2),\n",
       "    np.str_('medium'): np.float64(0.4),\n",
       "    np.str_('small'): np.float64(0.4)},\n",
       "   'levels': array(['large', 'medium', 'small'], dtype='<U32')},\n",
       "  4: {'type': 'level',\n",
       "   'probs': {np.str_('0'): np.float64(0.6666666666666666),\n",
       "    np.str_('1'): np.float64(0.3333333333333333)},\n",
       "   'levels': array(['0', '1'], dtype='<U32')}},\n",
       " np.int64(2): {0: {'type': 'continuous',\n",
       "   'mean': np.float64(19.466666666666665),\n",
       "   'std': np.float64(1.0964599468932352)},\n",
       "  1: {'type': 'continuous',\n",
       "   'mean': np.float64(66.0),\n",
       "   'std': np.float64(2.160247899469287)},\n",
       "  2: {'type': 'level',\n",
       "   'probs': {np.str_('no'): np.float64(0.6), np.str_('yes'): np.float64(0.4)},\n",
       "   'levels': array(['no', 'yes'], dtype='<U32')},\n",
       "  3: {'type': 'level',\n",
       "   'probs': {np.str_('large'): np.float64(0.6),\n",
       "    np.str_('medium'): np.float64(0.4)},\n",
       "   'levels': array(['large', 'medium'], dtype='<U32')},\n",
       "  4: {'type': 'level',\n",
       "   'probs': {np.str_('0'): np.float64(0.6), np.str_('1'): np.float64(0.4)},\n",
       "   'levels': array(['0', '1'], dtype='<U32')}}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f0df5-1765-4e7e-95ab-1707161358c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f0646-b415-43b2-85b1-4d223e115422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd8074-fa44-4df9-862c-3d7e7e9ba5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

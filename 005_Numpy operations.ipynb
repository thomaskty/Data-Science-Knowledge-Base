{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6909591-2248-46fd-8136-f77c25379f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150937fa-09e5-47a5-af62-adf0c49d4122",
   "metadata": {},
   "source": [
    "# array creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559eeb97-cd38-4037-a0f3-09b44c097122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Python lists\n",
    "a = np.array([1, 2, 3, 4])\n",
    "\n",
    "# 2D array (matrix)\n",
    "b = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "# Explicit dtype (important for ML & memory)\n",
    "c = np.array([1, 2, 3], dtype=np.float32)\n",
    "\n",
    "# Zeros, ones, constants\n",
    "zeros = np.zeros((3, 4))\n",
    "ones = np.ones((2, 3))\n",
    "full = np.full((2, 2), 7)\n",
    "\n",
    "# Identity matrix (used in linear algebra)\n",
    "identity = np.eye(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1602044-1af3-4832-a922-7c6b3762d52d",
   "metadata": {},
   "source": [
    "# random numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31588bb-dbd7-4b46-afbc-35fb4d123954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform random numbers\n",
    "rand_uniform = np.random.rand(3, 3)\n",
    "\n",
    "# Normal (Gaussian) distribution – used in NN weight initialization\n",
    "rand_normal = np.random.randn(3, 3)\n",
    "\n",
    "# Random integers\n",
    "rand_int = np.random.randint(0, 100, size=(4, 5))\n",
    "\n",
    "# Set seed (reproducibility – interviews LOVE this)\n",
    "np.random.seed(42)\n",
    "weights = np.random.randn(5, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8610e-3501-4792-b373-d5460583aede",
   "metadata": {},
   "source": [
    "# shape/size/reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c358cac-7397-4bc8-a5c1-bea30b71c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(12)\n",
    "\n",
    "# Shape and dimensions\n",
    "x.shape\n",
    "x.ndim\n",
    "x.size\n",
    "\n",
    "# Reshape (VERY COMMON)\n",
    "x_reshaped = x.reshape(3, 4)\n",
    "\n",
    "# Flatten (for ML models)\n",
    "x_flat = x_reshaped.flatten()\n",
    "\n",
    "# Another flatten method (view when possible)\n",
    "x_ravel = x_reshaped.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cdb0f-b005-41bd-9b1c-0b7d255701c6",
   "metadata": {},
   "source": [
    "# data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f22fd87-78cc-44e2-bc82-b54d31960b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 30],\n",
       "       [50, 60]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([[10, 20, 30],\n",
    "                 [40, 50, 60],\n",
    "                 [70, 80, 90]])\n",
    "\n",
    "# Element access\n",
    "data[0, 1]\n",
    "\n",
    "# Row / column slicing\n",
    "data[1, :]\n",
    "data[:, 2]\n",
    "\n",
    "# Sub-matrix slicing\n",
    "data[0:2, 1:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10aacf2-bab3-4dc2-aeec-d27a583448d7",
   "metadata": {},
   "source": [
    "# Boolean masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90cc0ebc-9e6a-4e7d-8f30-4ef01258317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array([45, 78, 90, 62, 88])\n",
    "\n",
    "# Boolean condition\n",
    "mask = scores >= 70\n",
    "\n",
    "# Filter values\n",
    "passed = scores[mask]\n",
    "\n",
    "# One-liner filtering\n",
    "top_scores = scores[scores > 80]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5443df8-0867-4a95-be56-fdaa5e20fc25",
   "metadata": {},
   "source": [
    "# vectorized operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "176111da-3f99-47b5-b881-ab68eca42aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array([10, 20, 30, 40])\n",
    "\n",
    "# Element-wise operations\n",
    "add = x + y\n",
    "sub = x - y\n",
    "mul = x * y\n",
    "div = x / y\n",
    "\n",
    "# Scalar operations\n",
    "scaled = x * 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5b938-f823-4080-9b82-b255ea495344",
   "metadata": {},
   "source": [
    "# Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72914634-2471-4689-835c-47a7145b6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "bias = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "# Bias addition (used in neural networks)\n",
    "Z = X + bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c11d493-2e62-4546-b6d3-13b07218cd93",
   "metadata": {},
   "source": [
    "# aggregation and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "018eed75-f0ab-43b7-9454-f6ec375caa9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20., 50.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([[10, 20, 30],\n",
    "                 [40, 50, 60]])\n",
    "\n",
    "# Basic statistics\n",
    "data.mean()\n",
    "data.std()\n",
    "data.var()\n",
    "data.sum()\n",
    "data.min()\n",
    "data.max()\n",
    "\n",
    "# Axis-wise (VERY IMPORTANT)\n",
    "data.mean(axis=0)   # column-wise\n",
    "data.mean(axis=1)   # row-wise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab8652-ebba-41e2-b350-cee365d6347f",
   "metadata": {},
   "source": [
    "# linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f156eda3-e886-461b-a037-4a07eb33b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "# Matrix multiplication\n",
    "C = A @ B\n",
    "C_dot = np.dot(A, B)\n",
    "\n",
    "# Transpose\n",
    "A_T = A.T\n",
    "\n",
    "# Determinant\n",
    "det_A = np.linalg.det(A)\n",
    "\n",
    "# Inverse (used in linear regression)\n",
    "A_inv = np.linalg.inv(A)\n",
    "\n",
    "# Eigenvalues & eigenvectors\n",
    "eig_vals, eig_vecs = np.linalg.eig(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3e6ba57-1352-4702-a62d-1b593233bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([3, 4])\n",
    "\n",
    "# L2 norm (Euclidean)\n",
    "l2 = np.linalg.norm(v)\n",
    "\n",
    "# L1 norm\n",
    "l1 = np.linalg.norm(v, ord=1)\n",
    "\n",
    "# Distance between vectors\n",
    "u = np.array([1, 2])\n",
    "distance = np.linalg.norm(v - u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f92e2-e3a5-42cf-8b3d-807f8718baea",
   "metadata": {},
   "source": [
    "# sorting and ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "610d9346-42be-4b23-a963-43c606483ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([40, 10, 30, 20])\n",
    "\n",
    "# Sort values\n",
    "sorted_arr = np.sort(arr)\n",
    "\n",
    "# Indices that would sort array\n",
    "sort_idx = np.argsort(arr)\n",
    "\n",
    "# Top-k values (common in ML)\n",
    "top2 = np.sort(arr)[-2:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b536f90-9fbe-44d3-b491-4fd6ea4242c6",
   "metadata": {},
   "source": [
    "# count & set operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d9d5a22-38e5-4974-9f32-7d51f60e313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([1, 2, 2, 3, 3, 3])\n",
    "\n",
    "# Unique values\n",
    "unique_vals = np.unique(labels)\n",
    "\n",
    "# Unique with counts (VERY COMMON)\n",
    "vals, counts = np.unique(labels, return_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272424e9-76d6-4428-9a52-074cdc07aa04",
   "metadata": {},
   "source": [
    "# missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da18ab9-a627-49a6-88a5-a7682b01d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1.0, 2.0, np.nan, 4.0])\n",
    "\n",
    "# Check NaNs\n",
    "np.isnan(data)\n",
    "\n",
    "# Remove NaNs\n",
    "clean_data = data[~np.isnan(data)]\n",
    "\n",
    "# Replace NaNs\n",
    "filled = np.nan_to_num(data, nan=0.0)\n",
    "\n",
    "# Aggregations ignoring NaNs\n",
    "np.nanmean(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa6ac9-283f-45a7-89c4-8486240f167e",
   "metadata": {},
   "source": [
    "# stacking and concatenations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7eecf4f-fb59-41ed-8ec5-8bab32178547",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "# Vertical stacking\n",
    "vstack = np.vstack((A, B))\n",
    "\n",
    "# Horizontal stacking\n",
    "hstack = np.hstack((A, B))\n",
    "\n",
    "# Generic concatenate\n",
    "concat = np.concatenate((A, B), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539bf53-4baa-4dc5-99c2-103adadc8740",
   "metadata": {},
   "source": [
    "# feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd1c546c-bb02-42a7-a104-389dfaa0ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 200],\n",
    "              [2, 300],\n",
    "              [3, 400]])\n",
    "\n",
    "# Min-Max Scaling\n",
    "X_min = X.min(axis=0)\n",
    "X_max = X.max(axis=0)\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "# Standardization (Z-score)\n",
    "mean = X.mean(axis=0)\n",
    "std = X.std(axis=0)\n",
    "X_standardized = (X - mean) / std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118d30e-307e-426f-b41c-a810fc83aa2c",
   "metadata": {},
   "source": [
    "# activation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db243678-ca6a-43e2-b112-7f236497fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ReLU\n",
    "relu = np.maximum(0, z)\n",
    "\n",
    "# Softmax (output layer)\n",
    "exp_z = np.exp(z - np.max(z))\n",
    "softmax = exp_z / np.sum(exp_z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db93c5-aaa6-4bc6-bafc-8a543482a2cd",
   "metadata": {},
   "source": [
    "# loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f1d9f14-77e1-4f82-9226-9a090a0e639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error\n",
    "y_true = np.array([3.0, 2.5, 4.0])\n",
    "y_pred = np.array([2.5, 2.0, 4.5])\n",
    "\n",
    "mse = np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Cross-entropy loss\n",
    "y_true = np.array([1, 0, 0])\n",
    "y_pred = np.array([0.7, 0.2, 0.1])\n",
    "\n",
    "cross_entropy = -np.sum(y_true * np.log(y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa61586-c1f7-4c91-8408-051a0a6a218b",
   "metadata": {},
   "source": [
    "# np.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f2bbf82-385b-448a-8eee-c139bd9f16e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 2.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# ReLU using np.where\n",
    "relu = np.where(x > 0, x, 0)\n",
    "relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5150bea6-d0e1-48be-987e-4101028460a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fail', 'Pass', 'Pass', 'Fail', 'Pass'], dtype='<U4')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = np.array([45, 78, 90, 62, 88])\n",
    "\n",
    "# Assign labels based on condition\n",
    "labels = np.where(scores >= 70, \"Pass\", \"Fail\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2912d85f-5f7b-4bfb-ad93-468df883e5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  0, 20,  0,  0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([10, -5, 20, -3, 0])\n",
    "\n",
    "# Replace negative values with 0\n",
    "cleaned = np.where(data < 0, 0, data)\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995151b-97aa-448d-9e38-768e4c087564",
   "metadata": {},
   "source": [
    "# views/actual copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "620e4590-f3b7-4d0e-bfe3-5bbbefa65916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 100 100]\n",
      "[  1 100 100 100   5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "slice_view = arr[1:4]\n",
    "slice_view[:] = 100\n",
    "\n",
    "print(slice_view)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f821a402-85a9-4a77-b421-e008dc2728c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "[200 200 200]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "slice_copy = arr[1:4].copy()\n",
    "slice_copy[:] = 200\n",
    "print(arr)\n",
    "\n",
    "print(slice_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250fdfa-4373-4202-934a-07e9f29d5d49",
   "metadata": {},
   "source": [
    "# clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e37e01d5-7c79-4142-989b-1df9595fa5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = np.array([0.0, 0.2, 0.8, 1.0])\n",
    "\n",
    "# Clip probabilities to avoid log(0)\n",
    "y_pred_safe = np.clip(y_pred, 1e-8, 1 - 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07db53ed-2dce-4c93-8352-4d643cea4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([10, 20, 300, 25, 30])\n",
    "\n",
    "# Cap values between 0 and 100\n",
    "features_clipped = np.clip(features, 0, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7eac6aa2-c588-4e9c-8ea2-1aa5387efad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = np.array([5.0, -12.0, 3.0, -8.0])\n",
    "\n",
    "# Prevent exploding gradients\n",
    "gradients_clipped = np.clip(gradients, -5.0, 5.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3dfe12-13fb-4216-96c1-979668872d4d",
   "metadata": {},
   "source": [
    "# Neural Network : Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1c42074-1d72-4d1e-8b05-2fbd899d7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simple dataset (2 features, binary labels)\n",
    "X = np.array([\n",
    "    [0.5, 1.5],\n",
    "    [1.0, 1.0],\n",
    "    [1.5, 0.5],\n",
    "    [3.0, 3.5],\n",
    "    [3.5, 3.0],\n",
    "    [4.0, 4.0]\n",
    "])\n",
    "\n",
    "y = np.array([[0], [0], [0], [1], [1], [1]])\n",
    "\n",
    "# Number of samples\n",
    "m = X.shape[0]\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    # a is sigmoid(z)\n",
    "    return a * (1 - a)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-8  # to avoid log(0)\n",
    "    return -np.mean(\n",
    "        y_true * np.log(y_pred + epsilon) +\n",
    "        (1 - y_true) * np.log(1 - y_pred + epsilon)\n",
    "    )\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        # Weight initialization (Gaussian)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Layer 1\n",
    "        self.Z1 = X @ self.W1 + self.b1\n",
    "        self.A1 = sigmoid(self.Z1)\n",
    "\n",
    "        # Output layer\n",
    "        self.Z2 = self.A1 @ self.W2 + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "\n",
    "        return self.A2\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Output layer gradients\n",
    "        dZ2 = self.A2 - y\n",
    "        dW2 = (1 / m) * (self.A1.T @ dZ2)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "        # Hidden layer gradients\n",
    "        dA1 = dZ2 @ self.W2.T\n",
    "        dZ1 = dA1 * sigmoid_derivative(self.A1)\n",
    "        dW1 = (1 / m) * (X.T @ dZ1)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradient descent update\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "\n",
    "\n",
    "    # 1. We compute the gradient of the loss w.r.t. weights using the chain rule, \n",
    "    # 2. propagate errors backward layer by layer, \n",
    "    # 3. and update parameters using gradient descent\n",
    "    \n",
    "    def train(self, X, y, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = binary_cross_entropy(y, y_pred)\n",
    "            self.backward(X, y)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probs = self.forward(X)\n",
    "        return (probs >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81b0c7b4-ae3f-470d-aa30-d1a23e17ec99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6932\n",
      "Epoch 100, Loss: 0.2996\n",
      "Epoch 200, Loss: 0.0518\n",
      "Epoch 300, Loss: 0.0224\n",
      "Epoch 400, Loss: 0.0135\n",
      "Epoch 500, Loss: 0.0095\n",
      "Epoch 600, Loss: 0.0072\n",
      "Epoch 700, Loss: 0.0058\n",
      "Epoch 800, Loss: 0.0048\n",
      "Epoch 900, Loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "nn = NeuralNetwork(\n",
    "    input_size=2,\n",
    "    hidden_size=4,\n",
    "    output_size=1,\n",
    "    learning_rate=0.5\n",
    ")\n",
    "\n",
    "# Train\n",
    "nn.train(X, y, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c0bf187-3a19-4df6-a748-75c842ae11a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      " [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "predictions = nn.predict(X)\n",
    "print(\"Predictions:\\n\", predictions)\n",
    "print(\"Actual:\\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d243bb-9f7c-44fe-a1bf-398c4fbee70c",
   "metadata": {},
   "source": [
    "# Transformer Encoder output : Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60ff8813-d510-48da-b7c6-445684962ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.34903236,   1.59306367,   5.97405543, -10.07031512,\n",
       "          2.02015256,   0.08518497, -23.75934035, -10.59915599],\n",
       "       [ -1.66032544,  -2.04324826,  -1.20655615, -14.04199566,\n",
       "          1.9208312 ,   4.09177349, -27.47093388,  -5.75217534],\n",
       "       [ -2.7502641 ,   7.09734995,  -9.5584203 ,  -1.2801011 ,\n",
       "         18.56648003, -12.37400589,  21.17231967,  29.18579374],\n",
       "       [ -1.98435411,   1.3733636 ,  -7.7065881 ,  24.94931337,\n",
       "        -31.03617656,  15.301169  ,  30.50316471, -12.4812306 ],\n",
       "       [  3.08908097,   2.9440524 ,   8.55491482,  19.86921318,\n",
       "        -13.11686279,  -1.36116844,  25.60197251,  -9.20708819],\n",
       "       [ -1.9774402 ,   1.32215887,  -7.5889322 ,  24.28450688,\n",
       "        -30.44183046,  15.08031294,  29.53045714, -12.34457851]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Reproducibility\n",
    "# -------------------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Input sentence (token IDs)\n",
    "# -------------------------------\n",
    "# Assume sentence already tokenized\n",
    "tokens = np.array([4, 12, 7, 9, 3, 10])  # length = 6\n",
    "\n",
    "seq_len = len(tokens)\n",
    "vocab_size = 50\n",
    "d_model = 8  # embedding dimension\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Token Embeddings\n",
    "# -------------------------------\n",
    "# Learned embedding matrix\n",
    "token_embedding_matrix = np.random.randn(vocab_size, d_model)\n",
    "\n",
    "# Lookup embeddings\n",
    "token_embeddings = token_embedding_matrix[tokens]\n",
    "# Shape: (seq_len, d_model)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Positional Embeddings\n",
    "# -------------------------------\n",
    "# Learned positional embeddings\n",
    "positional_embedding_matrix = np.random.randn(seq_len, d_model)\n",
    "\n",
    "positional_embeddings = positional_embedding_matrix\n",
    "# Shape: (seq_len, d_model)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Encoder Input\n",
    "# -------------------------------\n",
    "# Token + position embeddings\n",
    "X = token_embeddings + positional_embeddings\n",
    "# Shape: (seq_len, d_model)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Weight matrices for Q, K, V\n",
    "# -------------------------------\n",
    "W_Q = np.random.randn(d_model, d_model)\n",
    "W_K = np.random.randn(d_model, d_model)\n",
    "W_V = np.random.randn(d_model, d_model)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Create Queries, Keys, Values\n",
    "# -------------------------------\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "# Shapes:\n",
    "# Q, K, V -> (seq_len, d_model)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Scaled Dot-Product Attention\n",
    "# -------------------------------\n",
    "\n",
    "# Dot product between Q and Kᵀ\n",
    "scores = Q @ K.T\n",
    "# Shape: (seq_len, seq_len)\n",
    "\n",
    "# Scale by sqrt(d_model)\n",
    "scaled_scores = scores / np.sqrt(d_model)\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Softmax (Attention Weights)\n",
    "# -------------------------------\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores)\n",
    "# Shape: (seq_len, seq_len)\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Weighted sum of Values\n",
    "# -------------------------------\n",
    "attention_output = attention_weights @ V\n",
    "# Shape: (seq_len, d_model)\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Output Projection\n",
    "# -------------------------------\n",
    "W_O = np.random.randn(d_model, d_model)\n",
    "\n",
    "encoder_output = attention_output @ W_O\n",
    "# Shape: (seq_len, d_model)\n",
    "\n",
    "# -------------------------------\n",
    "# Final Encoder Output\n",
    "# -------------------------------\n",
    "encoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61dcb24-aadc-4e6c-b9d3-860e07d7b6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f477a2-3b12-4344-a2cd-dfb5a067f25d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

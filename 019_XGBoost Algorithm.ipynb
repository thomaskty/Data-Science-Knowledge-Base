{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461055cc-f363-4653-b798-59636a5d9d22",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf787d8-82e0-470b-996d-5299a972dbd4",
   "metadata": {},
   "source": [
    "XGBoost is a regularised, second order gradient boosting with trees, engineered for scale. \n",
    "\n",
    "Conceptually; \n",
    "\n",
    "- Same additive model as Gradient Boosting\n",
    "- Same idea : fit corrections\n",
    "- Uses 2nd order Taylor expansion\n",
    "- Explicit Regularisation\n",
    "- Optimizes tree structure + leaf values jointly\n",
    "\n",
    "We build an additive model; \n",
    "\n",
    "$$\n",
    "\\hat{y_i} = F(x_i) = \\sum_{t=1}^{T} f_t(x_i)\n",
    "$$\n",
    "\n",
    "Each f_t is a tree and each tree maps an input to a leaf value. \n",
    "\n",
    "XGBoost objective is as follows; \n",
    "\n",
    "$$\n",
    "\\begin{align*}\\mathcal{L}&=\\sum_{i=1}^n L(y_i, \\hat{y}_i)+\\sum_{t=1}^T \\Omega(f_t)\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\Omega(f)&=\\gamma T+\\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2\\end{align*}\n",
    "$$\n",
    "\n",
    "Here $\\gamma$ is the penalty per leaf ( tree complexity) and $\\lambda$ is the L2 penalty on leaf weights. This regularisation term is a huge difference from vanilla GBM. \n",
    "\n",
    "This penalizes too many leaves ( prevents deep trees) and large leaf values - prevents aggressive corrections. Don’t add a tree unless it helps enough to justify its complexity. \n",
    "\n",
    "At iteration t, - step-wise learning ( one tree at a time) \n",
    "\n",
    "$$\n",
    "F^{t}(x) = F^{(t-1)}(x) + f_t(x)\n",
    "$$\n",
    "\n",
    "We want to choose $f_t$ to minimize; \n",
    "\n",
    "$$\n",
    "\\begin{align*}\\mathcal{L}^{(t)}&=\\sum_i L\\left(y_i, F^{(t-1)}(x_i) + f_t(x_i)\\right)+\\Omega(f_t)\\end{align*}\n",
    "$$\n",
    "\n",
    "What small function should I add to improve predictions?This is the gradient boosting logic.  \n",
    "\n",
    "---\n",
    "\n",
    "**XGBOOST - Stat-quest Notes - Regression**\n",
    "\n",
    "- We start with an initial prediction (0.5) regardless of regression or classification\n",
    "- We compute the residuals and fits a regression tree to the residuals  ( using xgboost tree)\n",
    "- How we build a xgboost tree for regression ?\n",
    "\n",
    "Each tree starts with a leaf of all the residuals; \n",
    "\n",
    "$$\n",
    "\\text{Similarity Score} = \\frac{\\text{Sum of Residuals,Squared}}{\\text{Number of Residuals}+\\lambda}\n",
    "$$\n",
    "\n",
    "We cluster similar residuals into two groups. \n",
    "\n",
    "How much better the leaves cluster similar residuals than the root node? We do this by calculating the Gain of splitting the residuals into two groups. \n",
    "\n",
    "$$\n",
    "\\text{Gain} = (\\text{Left}_{similarity} + \\text{Right}_{similarity}) - \\text{Root}_{similarity}\n",
    "$$\n",
    "\n",
    "We pick a parameter called gamma ( $\\gamma$ ) and we compare this with the gain. If the difference between the Gain and gamma is negative ( $\\text{Gain}-\\gamma$) , we will remove the branch. This is how we prune the tree. If we increase the gamma, the pruning will be extreme and we may end up in the initial prediction itself.\n",
    "\n",
    "Lambda, is a Regularisation parameter, which means that it is intended to reduce the prediction’s sensitivity to individual observations. If lambda is ≥0, the similarity scores are smaller. \n",
    "\n",
    "$$\n",
    "\\text{Output Value} = \\frac{\\text{Sum of Residuals}}{\\text{Number of Residuals}+\\lambda}\n",
    "$$\n",
    "\n",
    "The output value equation is like the similarity score except we do not square the sum of the residuals. \n",
    "\n",
    "Learning rate of xgboost is 0.3 ( by default ).\n",
    "\n",
    "We keep building trees until the Residuals are super small, or we have reached the maximum number. \n",
    "\n",
    "**XGBOOST - Stat-quest Notes - Classification**\n",
    "\n",
    "We start with the initial prediction 0.5\n",
    "\n",
    "We have a new formula for similarity scores in classification.\n",
    "\n",
    "$$\n",
    "\\frac{\\text{Sum of Residuals,Squared}}{\\sum \\left[ \\text{Previous Probability}*(1-\\text{Previous Probability})\\right]+ \\lambda}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Cover = {\\sum \\left[ \\text{Previous Probability}*(1-\\text{Previous Probability})\\right]}\n",
    "$$\n",
    "\n",
    "In regression, cover is just the number of residuals in the leaf. \n",
    "\n",
    "We need to convert this probability into log(odds) value. \n",
    "\n",
    "and add this with the tree output; then we get the log(odds) value. \n",
    "\n",
    "Now we convert this into probability. and we got new residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7160ba-7a01-4f1a-b08d-edfe77070609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

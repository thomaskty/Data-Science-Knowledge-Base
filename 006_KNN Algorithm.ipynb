{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581a48df-92b6-46d7-a8a3-79538371b15b",
   "metadata": {},
   "source": [
    "# K - nearest neighbor Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04691dc-42f3-44ea-b343-589dd5d6a9b6",
   "metadata": {},
   "source": [
    "* Non parametric, instance -based , lazy learning algorithm. \n",
    "* Lazy - No training phase, all work at prediction time\n",
    "* Instance -based - predictions use the training data directly.\n",
    "* Non-parametric - no fixed finite parameter vector. In KNN the model is the metric + the value of k. There are no learned parameters like in linear regression.\n",
    "* Local averaging estimator\n",
    "* **Given a query point $x$; we compute the distances $d(x,x_i)$ and then select the $k$ nearest points; $N_k(x)$. Predict using only those points.**\n",
    "\n",
    "\n",
    "$$\n",
    "\\Large \\hat f(x)=\\frac{1}{k}\\sum_{i \\in \\mathcal{N}_k(x)} y_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Large \\hat y(x)=\\arg\\max_{c}\\sum_{i \\in \\mathcal{N}_k(x)} \\mathbb{I}(y_i = c)\n",
    "$$\n",
    "\n",
    "- If we choose small k , very local, sensitive to noise\n",
    "- If we choose large k, heavy averaging, over-smoothing\n",
    "- Feature scaling is mandatory.\n",
    "- KNN works because of local smoothness; points close in input space tends to ahve similar outputs. As dimension increases, all distances becomes similar.\n",
    "- Mahalanobis distance handles correlated features\n",
    "- Feature scaling, correlated features, mixed data types, curse of dimensionality.\n",
    "\n",
    "$$\n",
    "\\Large d_M(x, z)=\\sqrt{(x - z)^T \\Sigma^{-1} (x - z)}\n",
    "$$\n",
    "\n",
    "* when we take the normal difference $x-z$ . It does not tell you how surprising that difference it ; so surprise depends on how variable the data is and in which direction data naturally varies. so distance must be normalized by variance. \n",
    "* Mahalanobis distance answers how many standard deviations away is one point from another, considering the dataâ€™s natural variation. \n",
    "* If we remove the correlations and makes variances equal to 1, then Mahalanobis distance is Euclidean distance in whitened space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31941ad-8123-4b81-af2b-8a9a6c711ae5",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fbc6eb-1552-4694-a1b4-dadcb07e5d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Toy dataset\n",
    "X_train = np.array([\n",
    "    [1.0, 2.0],\n",
    "    [2.0, 3.0],\n",
    "    [3.0, 3.0],\n",
    "    [6.0, 5.0],\n",
    "    [7.0, 7.0],\n",
    "    [8.0, 6.0]\n",
    "])\n",
    "\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])  # Binary labels\n",
    "\n",
    "# Query point\n",
    "x_query = np.array([5.0, 5.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a79427-42fa-4294-acfe-39708ba34aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21065f5a-b1b6-488a-874f-e10e0c34b7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.        , 3.60555128, 2.82842712, 1.        , 2.82842712,\n",
       "       3.16227766])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute distances to all training points\n",
    "distances = np.array([euclidean_distance(x_query, x) for x in X_train])\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ff72f08-2fe0-4d7e-bd87-9edc39670f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(1), array([3, 2, 4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knn_predict(X_train, y_train, x_query, k=3):\n",
    "    # Compute distances\n",
    "    distances = np.sqrt(np.sum((X_train - x_query) ** 2, axis=1))\n",
    "    \n",
    "    # Get indices of k nearest neighbors\n",
    "    nearest_idx = np.argsort(distances)[:k]\n",
    "    \n",
    "    # Classification (majority vote)\n",
    "    nearest_labels = y_train[nearest_idx]\n",
    "    counts = np.bincount(nearest_labels)\n",
    "    predicted_class = np.argmax(counts)\n",
    "    \n",
    "    return predicted_class, nearest_idx\n",
    "\n",
    "pred_class, nearest_idx = knn_predict(X_train, y_train, x_query, k=3)\n",
    "pred_class, nearest_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68f1ffe8-381f-4b0d-86b9-ef4304fe3802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(4.533333333333333), array([3, 2, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_reg = np.array([1.2, 1.9, 2.1, 5.0, 6.5, 5.5])  # Continuous outputs\n",
    "\n",
    "def knn_regression(X_train, y_train, x_query, k=3):\n",
    "    distances = np.sqrt(np.sum((X_train - x_query) ** 2, axis=1))\n",
    "    nearest_idx = np.argsort(distances)[:k]\n",
    "    return np.mean(y_train[nearest_idx]), nearest_idx\n",
    "\n",
    "pred_value, nearest_idx = knn_regression(X_train, y_train_reg, x_query, k=3)\n",
    "pred_value, nearest_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2534a578-9d1f-47c8-9ecc-1042c5c6cdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([300.02666548, 200.02249873, 250.00799987,   1.        ,\n",
       "        100.019998  ,  50.08991915]),\n",
       " array([0.9428842 , 0.65853889, 0.68721005, 0.14285714, 0.37964806,\n",
       "        0.44642857]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppose second feature has higher scale\n",
    "X_train_scaled = np.array([\n",
    "    [1.0, 200.0],\n",
    "    [2.0, 300.0],\n",
    "    [3.0, 250.0],\n",
    "    [6.0, 500.0],\n",
    "    [7.0, 600.0],\n",
    "    [8.0, 550.0]\n",
    "])\n",
    "\n",
    "# Query point\n",
    "x_query_scaled = np.array([5.0, 500.0])\n",
    "\n",
    "# Euclidean distance without scaling\n",
    "distances_unscaled = np.sqrt(np.sum((X_train_scaled - x_query_scaled) ** 2, axis=1))\n",
    "\n",
    "# Min-Max scaling\n",
    "X_train_norm = (X_train_scaled - X_train_scaled.min(axis=0)) / (X_train_scaled.max(axis=0) - X_train_scaled.min(axis=0))\n",
    "x_query_norm = (x_query_scaled - X_train_scaled.min(axis=0)) / (X_train_scaled.max(axis=0) - X_train_scaled.min(axis=0))\n",
    "\n",
    "# Euclidean distance after scaling\n",
    "distances_scaled = np.sqrt(np.sum((X_train_norm - x_query_norm) ** 2, axis=1))\n",
    "\n",
    "distances_unscaled, distances_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d67850-f82d-4d6f-a507-518eb9f17ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5411035 , 1.04446594, 1.36515067, 1.14812099, 1.36515067,\n",
       "       1.90990242])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "# Covariance matrix of training data\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "cov_inv = inv(cov_matrix)\n",
    "\n",
    "def mahalanobis_distance(x, data, cov_inv):\n",
    "    diffs = data - x\n",
    "    return np.sqrt(np.diag(diffs @ cov_inv @ diffs.T))\n",
    "\n",
    "maha_distances = mahalanobis_distance(x_query, X_train, cov_inv)\n",
    "maha_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4631a3e-61da-40f7-9031-d46eba6993c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0), array([1, 3, 2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knn_mahalanobis(X_train, y_train, x_query, k=3):\n",
    "    cov_matrix = np.cov(X_train.T)\n",
    "    cov_inv = inv(cov_matrix)\n",
    "    distances = mahalanobis_distance(x_query, X_train, cov_inv)\n",
    "    nearest_idx = np.argsort(distances)[:k]\n",
    "    nearest_labels = y_train[nearest_idx]\n",
    "    counts = np.bincount(nearest_labels)\n",
    "    return np.argmax(counts), nearest_idx\n",
    "\n",
    "pred_class_maha, nearest_idx_maha = knn_mahalanobis(X_train, y_train, x_query, k=3)\n",
    "pred_class_maha, nearest_idx_maha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca660aa-8b26-4084-b237-6904307ba4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa4532-55e7-461c-b16f-8e108cbf5b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

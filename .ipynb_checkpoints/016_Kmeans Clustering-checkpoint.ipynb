{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfbb7da-5fbd-4a6f-9f25-6f8e253af521",
   "metadata": {},
   "source": [
    "**Formal Definition**: K-means clustering is an unsupervised learning algorithm that partitions a dataset $X={x_1,x_2,...,x_n}$ into $K$ disjoint clusters $C={c_1,c_2,...,c_k}$, where each clusters is represented by its centroid (mean) $\\mu_{k}$, with the objective of minimizing the within-cluster sum of squares.\n",
    "\n",
    "In K Means we partition data into K groups such that ;\n",
    "\n",
    "- Each data point belongs to exactly one cluster ( hard clustering)\n",
    "- Clusters are compact ( within cluster sum of squares should be as less as possible)\n",
    "- Each cluster is represented by a centroid( mean of the cluster points)\n",
    "\n",
    "*Group data into K clusters so that points are close to their own cluster centre and far from others*\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "- Inputs : Dataset with $n$ data points and the number of Clusters : $K$\n",
    "- Initialise centroids ( randomly or K-Means ++ )\n",
    "- Assignment Step : For each data point, compute its distance to each centroid and assign the point to the nearest centroid.\n",
    "- For each cluster - recalculate the centroid as the mean of all points assigned to that cluster.\n",
    "- Repeat this process, until the centroids do not change significantly or assignments do not change or maximum number of iterations is reached.\n",
    "- Final Outputs : The data points and the associated cluster labels.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\Large\\underset{C_1,\\dots,C_K}{\\min}\\sum_{k=1}^{K}\\sum_{x_i \\in C_k}\\left\\| x_i - \\mu_k \\right\\|^2\\end{align*}\n",
    "$$\n",
    "\n",
    "KMeans is **Coordinate Descent**- assignment step is there for optimize the cluster labels, and update step is there for optimizing the centroids\n",
    "\n",
    "**Concerns**- regarding cluster shape, selection of distance metric, selection of evaluation metric, selection of number of clusters, scaling of data points, centroid initializations,\n",
    "\n",
    "**Computational Complexity**\n",
    "\n",
    "**Question**- Given a set of points assigned to a cluster, where should the centroid be placed so that the total squared distance from the points to the centroid is minimized? The centroid that minimizes the sum of squared euclidean distances is the mean of the points.\n",
    "\n",
    "**Why K-Means has local minima( Not guaranteed Global minimum)?**\n",
    "\n",
    "- The objective is non-convex\n",
    "- Cluster assignments are discrete (combinatorial)\n",
    "- The algorithm uses greedy alternating optimization\n",
    "- Initialization fixes the \"basin of attraction\"\n",
    "\n",
    "Once K-Means makes early assignment decisions, it cannot undo them globally - it only makes locally improving moves.\n",
    "\n",
    "- The number of possible clustering grows exponentially with data size.\n",
    "- Even if the objective is convex with respect to centroids, it is not convex with respect to assignments.\n",
    "- This is why K-Means ++ helps ( but doesn't solve it): improves the initialization, reduces the probability of bad local minima.\n",
    "\n",
    "**Pairwise distance to centroid based kmeans objective**\n",
    "\n",
    "- Pairwise View : All points inside a cluster should be close to each other\n",
    "- All points should be close to the cluster center\n",
    "- Minimizing the sum of squared distances between all pairs of points inside a cluster is equivalent to minimizing the sum of squared distances from points to the cluster centroid.\n",
    "\n",
    "**Variance Decomposition Implications**\n",
    "\n",
    "$TSS=WCSS+BCSS$. Since $TSS$ is constant for any given dataset, minimizing $WCSS$ is mathematically equivalent to maximizing $BCSS$. In other words, creating tight clusters automatically means creating well-separated clusters. - complementary\n",
    "\n",
    "**Computational Complexity**\n",
    "In the assignment step, we must compute the distance from each of the n points to each of the K centroids. For data in d dimensions, each distance computation requires $O(d)$ a operations. Thus, the assignment step has complexity $O(nKd)$\n",
    "\n",
    "In the update step, we must compute the mean of each cluster. This requires summing all points in each cluster and dividing by the cluster size, which takes $O(nd)$ operations in total.\n",
    "\n",
    "If the algorithm converges in I iterations, the total time complexity is ; $O(I.n.K.d)$\n",
    "\n",
    "## K-Means ++\n",
    "\n",
    "**K-means++ Initialization**- The key idea is to choose initial centroids that are far apart from each other.\n",
    "\n",
    "- Pick one data point uniformly at random.\n",
    "- For each data point , compute distance to nearest already chosen centroid.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\Large D(x_i)=\\min_{1 \\le j \\le t}\\|x_i - \\mu_j\\|\\end{align*}\n",
    "$$\n",
    "\n",
    "- When computing this distance, we are asking the question 'Is this point already well represented by an existing centroid'. if yes then the $D(x_i)$ will be small, otherwise large. Points with large distance are good candidates for new centroids.\n",
    "- Instead of always picking the farthest point (deterministic) K means ++ uses probabilistic sampling.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\Large P(x_i)=\\frac{D(x_i)^2}{\\sum_{j=1}^{n} D(x_j)^2}\\end{align*}\n",
    "$$\n",
    "\n",
    "**Why does kmeans ++ reduce sensitivity to outliers?**- Because it samples points proportional to squared distance, so dense regions collectively outweigh isolated extreme points, unlike deterministic farthest point selection.\n",
    "\n",
    "**K-Means Through the Expectation-Maximization Lens**\n",
    "\n",
    "Recalling K-Means objective;\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\Large\\min_{\\{z_{ik}, \\mu_k\\}}\\sum_{i=1}^{n} \\sum_{k=1}^{K}z_{ik} \\|x_i - \\mu_k\\|^2\\end{align*}\n",
    "$$\n",
    "\n",
    "Here $z_{ik}$ is 1 if $x_i$ belongs to cluster k and 0 otherwise.\n",
    "\n",
    "**Expectation Step**: Given current centroids $\\mu_{k}$;\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}z_{ik} =\\begin{cases}1 & k = \\arg\\min_j \\|x_i - \\mu_j\\|^2 \\\\0 & \\text{otherwise}\\end{cases}\\end{align*}\n",
    "$$\n",
    "\n",
    "This is hard assignment ; Each point belongs to exactly one cluster( Probabilities are 0 or 1)\n",
    "\n",
    "**Maximization Step**: Given assignments $z_{ik}$;\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}\\mu_{k}=\\frac{\\sum_{i=1}^{n} z_{ik} x_i}{\\sum_{i=1}^{n} z_{ik}}\\end{align*}\n",
    "$$\n",
    "\n",
    "This is called the mean update\n",
    "\n",
    "**Comparison of K-Means & GMM in terms of EM Algorithm**\n",
    "\n",
    "KMeans and GMM both aim to cluster data, but they differ in what they assume about the data.\n",
    "\n",
    "Kmeans focuses directly on partitioning the data into groups by minimizing distances. It does not try to model how the data was generated; it only cares about assigning each point to the closest cluster center. In this sense, it behaves like a discriminative method.\n",
    "\n",
    "GMM assumes that the data was generated by a mixture of several underlying sources, and each data point is produced by one of these sources. The goal is to learn these sources and estimate how likely each data point came from each one.\n",
    "Because of this, GMM are generative ; instead of just assigning clusters, they try to model the full data distribution and explain the data-generation process.\n",
    "\n",
    "**Discussion about the use of Euclidean distance in Kmeans.**\n",
    "\n",
    "Kmeans is build around a very simple idea ; Represent a group of points by a single representative point ; what does central even mean . The answers depends on the distance we use. Under Euclidean distance squared, something magical happens; The point that minimize the total squared distance to all points is the mean. This is not true for most other distances.\n",
    "\n",
    "**What breaks if we don't use euclidean distance**\n",
    "\n",
    "- Mean is no longer optimal centroid\n",
    "- No variance interpretation\n",
    "- No EM interpretation\n",
    "\n",
    "**What happens if we use Manhattan distance (L1)**\n",
    "\n",
    "Under manhattan distance, the point that minimizes total distance is the median, not the mean. Manhattan distance cares about absolute deviations. The median minimizes absolute error. This is why medians are robust to outliers. So clustering with Manhattan distance naturally leads to k-medians.\n",
    "\n",
    "**K-Medoid : When the Center Must be a real point**\n",
    "\n",
    "In many real-world cases, distances are not euclidean, data will be mixed or categorical. You want a real data point as the representative. You want robustness to outliers.\n",
    "Medoid is an actual data point whose total distance to other points in the cluster is minimal.\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}\\min_{\\{m_k\\}} \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} d(x_i, m_k)\\end{align*}\n",
    "$$\n",
    "\n",
    "- Choose K data points as initial medoids ( random or heuristic)\n",
    "- Assign each point to the nearest medoid using the chosen distance\n",
    "- For each cluster;\n",
    "    - Try every point in the cluster as a candidate medoid\n",
    "    - Choose the one that minimizes total distance to others. ( new medoid)\n",
    "- Repeat until convergence\n",
    "- We donâ€™t need any geometry for the distance metric ( as long as smaller distance = more similar)\n",
    "- Expensive : Updating medoids requires pairwise distances. So in practice, people use; PAM (Partitioning Around Medoids) , CLARA ( sampling-based), CLARANS (randomized)\n",
    "\n",
    "**K-Modes ( For categorical data)**\n",
    "\n",
    "- For categorical attribute, distance is total Number of mismatched attributes.\n",
    "- The centroid is the mode;- Which is computed for each attribute.\n",
    "\n",
    "**K-Prototype( Numeric + Categorical)**\n",
    "\n",
    "- Combines k-means and k-modes in a single objective\n",
    "- Use Euclidean distance for numeric features\n",
    "- Use Mismatch distance for categorical features\n",
    "- Balance then with a weight\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}d(x, \\mu) =\\sum_{j \\in \\text{num}} (x_j - \\mu_j)^2\\;+\\;\\gamma \\sum_{j \\in \\text{cat}} \\mathbf{1}(x_j \\neq \\mu_j)\\end{align*}\n",
    "$$\n",
    "\n",
    "- The centroid has two parts: Numeric attributes will be updated using the mean. Categorical part will be updated using the mode.\n",
    "- Think of $\\gamma$ as answering : How much numeric difference is equivalent to one categorical difference?\n",
    "\n",
    "**Assumptions of KMeans**\n",
    "\n",
    "- Clusters are compact and well separated\n",
    "- Variance within clusters is roughly similar\n",
    "- Features are on comparable scales\n",
    "- Euclidean geometry is meaningful\n",
    "- Clusters are convex\n",
    "\n",
    "**Scaling and High-Dimensional Behaviour**\n",
    "\n",
    "- Curse of dimensionality effect on Euclidean distance\n",
    "- PCA can be applied\n",
    "\n",
    "**KMeans as Matrix Factorization**\n",
    "\n",
    "**Online / Mini-Batch kmeans**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9332982-62f3-46db-af20-35acfc9d66f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, centroid shift = 0.391578\n",
      "Iteration 1, centroid shift = 0.000000\n",
      "Convergence reached!\n",
      "Cluster labels: [0 0 1 1 0 1]\n",
      "Centroids (numeric): [[5.         3.26666667]\n",
      " [6.06666667 3.33333333]]\n",
      "Centroids (categorical): [['blue']\n",
      " ['green']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, n_clusters=3, max_iter=100, tol=1e-4, init='kmeans++', verbose=False):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.init = init\n",
    "        self.verbose = verbose\n",
    "        self.centroids = None\n",
    "        self.labels_ = None\n",
    "\n",
    "    # Euclidean distance\n",
    "    def _distance(self, x, y):\n",
    "        return np.linalg.norm(x - y)\n",
    "\n",
    "    # KMeans++ initialization\n",
    "    def _init_centroids(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        centroids = []\n",
    "\n",
    "        # Randomly pick first centroid\n",
    "        first_idx = np.random.randint(n_samples)\n",
    "        centroids.append(X[first_idx])\n",
    "        if self.verbose:\n",
    "            print(f\"Initial centroid 0: {centroids[0]}\")\n",
    "\n",
    "        for k in range(1, self.n_clusters):\n",
    "            distances = np.array([min([self._distance(x, c) for c in centroids]) for x in X])\n",
    "            probs = distances**2 / np.sum(distances**2)\n",
    "            cumulative_probs = np.cumsum(probs)\n",
    "            r = np.random.rand()\n",
    "            next_idx = np.searchsorted(cumulative_probs, r)\n",
    "            centroids.append(X[next_idx])\n",
    "            if self.verbose:\n",
    "                print(f\"Chosen centroid {k}: {centroids[k]}\")\n",
    "        return np.array(centroids)\n",
    "\n",
    "    # Assign points to closest centroid\n",
    "    def _assign_clusters(self, X):\n",
    "        labels = np.array([np.argmin([self._distance(x, c) for c in self.centroids]) for x in X])\n",
    "        return labels\n",
    "\n",
    "    # Update centroids based on cluster assignment\n",
    "    def _update_centroids(self, X, labels):\n",
    "        new_centroids = np.zeros_like(self.centroids)\n",
    "        for k in range(self.n_clusters):\n",
    "            cluster_points = X[labels == k]\n",
    "            if len(cluster_points) > 0:\n",
    "                new_centroids[k] = cluster_points.mean(axis=0)\n",
    "            else:\n",
    "                # Reinitialize empty cluster randomly\n",
    "                new_centroids[k] = X[np.random.randint(0, X.shape[0])]\n",
    "        return new_centroids\n",
    "\n",
    "    # Fit the KMeans model\n",
    "    def fit(self, X):\n",
    "        X = np.array(X)\n",
    "        if self.init == 'kmeans++':\n",
    "            self.centroids = self._init_centroids(X)\n",
    "        else:\n",
    "            indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)\n",
    "            self.centroids = X[indices]\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            labels = self._assign_clusters(X)\n",
    "            new_centroids = self._update_centroids(X, labels)\n",
    "\n",
    "            # Convergence check\n",
    "            diff = np.linalg.norm(new_centroids - self.centroids)\n",
    "            if self.verbose:\n",
    "                print(f\"Iteration {i}: centroid shift = {diff:.6f}\")\n",
    "            if diff < self.tol:\n",
    "                if self.verbose:\n",
    "                    print(\"Convergence reached!\")\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "        self.labels_ = labels\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return self._assign_clusters(X)\n",
    "\n",
    "# ----------------------------\n",
    "# K-Prototypes: numeric + categorical\n",
    "# ----------------------------\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "class KPrototypes:\n",
    "    def __init__(self, n_clusters=2, max_iter=100, tol=1e-4, gamma=1.0, verbose=False):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.gamma = gamma\n",
    "        self.verbose = verbose\n",
    "        self.centroids = None\n",
    "        self.labels_ = None\n",
    "\n",
    "    # distance: numeric = Euclidean, categorical = mismatch\n",
    "    def _distance(self, x_num, x_cat, c_num, c_cat):\n",
    "        num_dist = np.linalg.norm(x_num - c_num)\n",
    "        cat_dist = np.sum(x_cat != c_cat)\n",
    "        return num_dist + self.gamma * cat_dist\n",
    "\n",
    "    # initialize centroids (random sample)\n",
    "    def _init_centroids(self, X_num, X_cat):\n",
    "        n_samples = X_num.shape[0]\n",
    "        indices = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
    "        c_num = X_num[indices]\n",
    "        c_cat = X_cat[indices]\n",
    "        return c_num, c_cat\n",
    "\n",
    "    # assign clusters\n",
    "    def _assign_clusters(self, X_num, X_cat, c_num, c_cat):\n",
    "        labels = []\n",
    "        for x_n, x_c in zip(X_num, X_cat):\n",
    "            dists = [self._distance(x_n, x_c, cn, cc) for cn, cc in zip(c_num, c_cat)]\n",
    "            labels.append(np.argmin(dists))\n",
    "        return np.array(labels)\n",
    "\n",
    "    # update centroids\n",
    "    def _update_centroids(self, X_num, X_cat, labels):\n",
    "        new_c_num = np.zeros((self.n_clusters, X_num.shape[1]))\n",
    "        new_c_cat = np.zeros((self.n_clusters, X_cat.shape[1]), dtype=object)\n",
    "        for k in range(self.n_clusters):\n",
    "            points_num = X_num[labels == k]\n",
    "            points_cat = X_cat[labels == k]\n",
    "            if len(points_num) > 0:\n",
    "                new_c_num[k] = points_num.mean(axis=0)\n",
    "                for j in range(points_cat.shape[1]):\n",
    "                    counts = Counter(points_cat[:, j])\n",
    "                    new_c_cat[k, j] = counts.most_common(1)[0][0]\n",
    "            else:\n",
    "                idx = np.random.randint(0, X_num.shape[0])\n",
    "                new_c_num[k] = X_num[idx]\n",
    "                new_c_cat[k] = X_cat[idx]\n",
    "        return new_c_num, new_c_cat\n",
    "\n",
    "    def fit(self, X, num_cols, cat_cols):\n",
    "        # separate numeric and categorical columns\n",
    "        X = np.array(X, dtype=object)\n",
    "        X_num = X[:, num_cols].astype(float)\n",
    "        X_cat = X[:, cat_cols].astype(object)\n",
    "\n",
    "        # initialize centroids\n",
    "        c_num, c_cat = self._init_centroids(X_num, X_cat)\n",
    "\n",
    "        for it in range(self.max_iter):\n",
    "            labels = self._assign_clusters(X_num, X_cat, c_num, c_cat)\n",
    "            new_c_num, new_c_cat = self._update_centroids(X_num, X_cat, labels)\n",
    "\n",
    "            shift = np.linalg.norm(new_c_num - c_num)\n",
    "            if self.verbose:\n",
    "                print(f\"Iteration {it}, centroid shift = {shift:.6f}\")\n",
    "            if shift < self.tol:\n",
    "                if self.verbose:\n",
    "                    print(\"Convergence reached!\")\n",
    "                break\n",
    "\n",
    "            c_num, c_cat = new_c_num, new_c_cat\n",
    "\n",
    "        self.centroids = (c_num, c_cat)\n",
    "        self.labels_ = labels\n",
    "\n",
    "    def predict(self, X, num_cols, cat_cols):\n",
    "        X = np.array(X, dtype=object)\n",
    "        X_num = X[:, num_cols].astype(float)\n",
    "        X_cat = X[:, cat_cols].astype(object)\n",
    "        c_num, c_cat = self.centroids\n",
    "        return self._assign_clusters(X_num, X_cat, c_num, c_cat)\n",
    "\n",
    "# ----------------------------\n",
    "# Example run\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # small mixed dataset: numeric + categorical\n",
    "    X_mixed = np.array([\n",
    "        [5.1, 3.5, 'red'],\n",
    "        [4.9, 3.0, 'blue'],\n",
    "        [6.2, 3.4, 'red'],\n",
    "        [5.9, 3.0, 'green'],\n",
    "        [5.0, 3.3, 'blue'],\n",
    "        [6.1, 3.6, 'green']\n",
    "    ])\n",
    "    num_cols = [0, 1]\n",
    "    cat_cols = [2]\n",
    "\n",
    "    kp = KPrototypes(n_clusters=2, verbose=True, gamma=1.0)\n",
    "    kp.fit(X_mixed, num_cols=num_cols, cat_cols=cat_cols)\n",
    "    print(\"Cluster labels:\", kp.labels_)\n",
    "    print(\"Centroids (numeric):\", kp.centroids[0])\n",
    "    print(\"Centroids (categorical):\", kp.centroids[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094fc19c-56ab-41e2-b95a-d82b9290836d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
